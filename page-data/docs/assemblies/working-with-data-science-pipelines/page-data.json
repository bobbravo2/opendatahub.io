{"componentChunkName":"component---src-templates-docs-page-tsx","path":"/docs/assemblies/working-with-data-science-pipelines/","result":{"data":{"allFile":{"edges":[{"node":{"childAsciidoc":{"fields":{"slug":"/docs/getting-started-with-open-data-hub/"},"sections":[{"parentId":null,"name":"Logging in to Open Data Hub","level":1,"index":0,"id":"logging-in_get-started"},{"parentId":null,"name":"The Open Data Hub user interface","level":1,"index":1,"id":"user-interface_get-started"},{"parentId":"user-interface_get-started","name":"Global navigation","level":2,"index":0,"id":"_global_navigation"},{"parentId":"user-interface_get-started","name":"Side navigation","level":2,"index":1,"id":"_side_navigation"},{"parentId":null,"name":"Notifications in Open Data Hub","level":1,"index":2,"id":"notifications_get-started"},{"parentId":null,"name":"Creating a data science project","level":1,"index":3,"id":"creating-a-data-science-project_get-started"},{"parentId":null,"name":"Creating a project workbench","level":1,"index":4,"id":"creating-a-project-workbench_get-started"},{"parentId":"creating-a-project-workbench_get-started","name":"Launching Jupyter and starting a notebook server","level":2,"index":0,"id":"launching-jupyter-and-starting-a-notebook-server_get-started"},{"parentId":"creating-a-project-workbench_get-started","name":"Options for notebook server environments","level":2,"index":1,"id":"options-for-notebook-server-environments_get-started"},{"parentId":null,"name":"Tutorials for data scientists","level":1,"index":5,"id":"tutorials-for-data-scientists_get-started"},{"parentId":"tutorials-for-data-scientists_get-started","name":"Accessing tutorials","level":2,"index":0,"id":"accessing-tutorials_get-started"},{"parentId":null,"name":"Enabling services connected to Open Data Hub","level":1,"index":6,"id":"enabling-services_get-started"},{"parentId":null,"name":"Disabling applications connected to Open Data Hub","level":1,"index":7,"id":"disabling-applications_get-started"},{"parentId":"disabling-applications_get-started","name":"Removing disabled applications from Open Data Hub","level":2,"index":0,"id":"removing-disabled-applications_get-started"},{"parentId":null,"name":"Support requirements and limitations","level":1,"index":8,"id":"support-requirements-and-limitations_requirements"},{"parentId":"support-requirements-and-limitations_requirements","name":"Supported browsers","level":2,"index":0,"id":"supported-browsers_requirements"},{"parentId":"support-requirements-and-limitations_requirements","name":"Supported services","level":2,"index":1,"id":"supported-services_requirements"},{"parentId":"support-requirements-and-limitations_requirements","name":"Supported packages","level":2,"index":2,"id":"supported-packages_requirements"},{"parentId":null,"name":"Common questions","level":1,"index":9,"id":"common-questions_get-started"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/working-on-data-science-projects/"},"sections":[{"parentId":null,"name":"Creating and importing notebooks","level":1,"index":0,"id":"creating-and-importing-notebooks_notebooks"},{"parentId":"creating-and-importing-notebooks_notebooks","name":"Creating a new notebook","level":2,"index":0,"id":"creating-a-new-notebook_notebooks"},{"parentId":"creating-a-new-notebook_notebooks","name":"Notebook images for data scientists","level":3,"index":0,"id":"notebook-images-for-data-scientists_notebooks"},{"parentId":"creating-and-importing-notebooks_notebooks","name":"Uploading an existing notebook file from local storage","level":2,"index":1,"id":"uploading-an-existing-notebook-file-from-local-storage_notebooks"},{"parentId":"creating-and-importing-notebooks_notebooks","name":"Uploading an existing notebook file from a Git repository using JupyterLab","level":2,"index":2,"id":"uploading-an-existing-notebook-file-from-a-git-repository-using-jupyterlab_notebooks"},{"parentId":"creating-and-importing-notebooks_notebooks","name":"Uploading an existing notebook file from a Git repository using the command line interface","level":2,"index":3,"id":"uploading-an-existing-notebook-file-from-a-git-repository-using-the-command-line-interface_notebooks"},{"parentId":null,"name":"Collaborating on notebooks using Git","level":1,"index":1,"id":"collaborating-on-notebooks-using-git_git-collab"},{"parentId":"collaborating-on-notebooks-using-git_git-collab","name":"Uploading an existing notebook file from a Git repository using JupyterLab","level":2,"index":0,"id":"uploading-an-existing-notebook-file-from-a-git-repository-using-jupyterlab_git-collab"},{"parentId":"collaborating-on-notebooks-using-git_git-collab","name":"Uploading an existing notebook file from a Git repository using the command line interface","level":2,"index":1,"id":"uploading-an-existing-notebook-file-from-a-git-repository-using-the-command-line-interface_git-collab"},{"parentId":"collaborating-on-notebooks-using-git_git-collab","name":"Updating your project with changes from a remote Git repository","level":2,"index":2,"id":"updating-your-project-with-changes-from-a-remote-git-repository_git-collab"},{"parentId":"collaborating-on-notebooks-using-git_git-collab","name":"Pushing project changes to a Git repository","level":2,"index":3,"id":"pushing-project-changes-to-a-git-repository_git-collab"},{"parentId":null,"name":"Working on data science projects","level":1,"index":2,"id":"working-on-data-science-projects_nb-server"},{"parentId":"working-on-data-science-projects_nb-server","name":"Using data science projects","level":2,"index":0,"id":"_using_data_science_projects"},{"parentId":"_using_data_science_projects","name":"Creating a data science project","level":3,"index":0,"id":"creating-a-data-science-project_nb-server"},{"parentId":"_using_data_science_projects","name":"Updating a data science project","level":3,"index":1,"id":"updating-a-data-science-project_nb-server"},{"parentId":"_using_data_science_projects","name":"Deleting a data science project","level":3,"index":2,"id":"deleting-a-data-science-project_nb-server"},{"parentId":"working-on-data-science-projects_nb-server","name":"Using project workbenches","level":2,"index":1,"id":"_using_project_workbenches"},{"parentId":"_using_project_workbenches","name":"Creating a project workbench","level":3,"index":0,"id":"creating-a-project-workbench_nb-server"},{"parentId":"_using_project_workbenches","name":"Starting a workbench","level":3,"index":1,"id":"starting-a-workbench_nb-server"},{"parentId":"_using_project_workbenches","name":"Updating a project workbench","level":3,"index":2,"id":"updating-a-project-workbench_nb-server"},{"parentId":"_using_project_workbenches","name":"Deleting a workbench from a data science project","level":3,"index":3,"id":"deleting-a-workbench-from-a-data-science-project_nb-server"},{"parentId":"working-on-data-science-projects_nb-server","name":"Using data connections","level":2,"index":2,"id":"_using_data_connections"},{"parentId":"_using_data_connections","name":"Adding a data connection to your data science project","level":3,"index":0,"id":"adding-a-data-connection-to-your-data-science-project_nb-server"},{"parentId":"_using_data_connections","name":"Deleting a data connection","level":3,"index":1,"id":"deleting-a-data-connection_nb-server"},{"parentId":"_using_data_connections","name":"Updating a connected data source","level":3,"index":2,"id":"updating-a-connected-data-source_nb-server"},{"parentId":"working-on-data-science-projects_nb-server","name":"Configuring cluster storage","level":2,"index":3,"id":"_configuring_cluster_storage"},{"parentId":"_configuring_cluster_storage","name":"Adding cluster storage to your data science project","level":3,"index":0,"id":"adding-cluster-storage-to-your-data-science-project_nb-server"},{"parentId":"_configuring_cluster_storage","name":"Updating cluster storage","level":3,"index":1,"id":"updating-cluster-storage_nb-server"},{"parentId":"_configuring_cluster_storage","name":"Deleting cluster storage from a data science project","level":3,"index":2,"id":"deleting-cluster-storage-from-a-data-science-project_nb-server"},{"parentId":"working-on-data-science-projects_nb-server","name":"Configuring model servers","level":2,"index":4,"id":"_configuring_model_servers"},{"parentId":"_configuring_model_servers","name":"Adding a custom model-serving runtime","level":3,"index":0,"id":"adding-a-custom-model-serving-runtime_nb-server"},{"parentId":"_configuring_model_servers","name":"Updating a model server","level":3,"index":1,"id":"updating-a-model-server_nb-server"},{"parentId":"_configuring_model_servers","name":"Deleting a model server","level":3,"index":2,"id":"deleting-a-model-server_nb-server"},{"parentId":"working-on-data-science-projects_nb-server","name":"Configuring access to data science projects","level":2,"index":5,"id":"_configuring_access_to_data_science_projects"},{"parentId":"_configuring_access_to_data_science_projects","name":"Configuring access to data science projects","level":3,"index":0,"id":"configuring-access-to-data-science-projects_nb-server"},{"parentId":"_configuring_access_to_data_science_projects","name":"Sharing access to a data science project","level":3,"index":1,"id":"sharing-access-to-a-data-science-project_nb-server"},{"parentId":"_configuring_access_to_data_science_projects","name":"Updating access to a data science project","level":3,"index":2,"id":"updating-access-to-a-data-science-project_nb-server"},{"parentId":"_configuring_access_to_data_science_projects","name":"Removing access to a data science project","level":3,"index":3,"id":"removing-access-to-a-data-science-project_nb-server"},{"parentId":"working-on-data-science-projects_nb-server","name":"Viewing Python packages installed on your notebook server","level":2,"index":6,"id":"viewing-python-packages-installed-on-your-notebook-server_nb-server"},{"parentId":"working-on-data-science-projects_nb-server","name":"Installing Python packages on your notebook server","level":2,"index":7,"id":"installing-python-packages-on-your-notebook-server_nb-server"},{"parentId":"working-on-data-science-projects_nb-server","name":"Updating notebook server settings by restarting your server","level":2,"index":8,"id":"updating-notebook-server-settings-by-restarting-your-server_nb-server"},{"parentId":null,"name":"Working with data science pipelines","level":1,"index":3,"id":"working-with-data-science-pipelines_ds-pipelines"},{"parentId":"working-with-data-science-pipelines_ds-pipelines","name":"Managing data science pipelines","level":2,"index":0,"id":"_managing_data_science_pipelines"},{"parentId":"_managing_data_science_pipelines","name":"Configuring a pipeline server","level":3,"index":0,"id":"configuring-a-pipeline-server_ds-pipelines"},{"parentId":"_managing_data_science_pipelines","name":"Defining a pipeline","level":3,"index":1,"id":"defining-a-pipeline_ds-pipelines"},{"parentId":"_managing_data_science_pipelines","name":"Importing a data science pipeline","level":3,"index":2,"id":"importing-a-data-science-pipeline_ds-pipelines"},{"parentId":"_managing_data_science_pipelines","name":"Downloading a data science pipeline","level":3,"index":3,"id":"downloading-a-data-science-pipeline_ds-pipelines"},{"parentId":"_managing_data_science_pipelines","name":"Deleting a data science pipeline","level":3,"index":4,"id":"deleting-a-data-science-pipeline_ds-pipelines"},{"parentId":"_managing_data_science_pipelines","name":"Deleting a pipeline server","level":3,"index":5,"id":"deleting-a-pipeline-server_ds-pipelines"},{"parentId":"_managing_data_science_pipelines","name":"Viewing the details of a pipeline server","level":3,"index":6,"id":"viewing-the-details-of-a-pipeline-server_ds-pipelines"},{"parentId":"_managing_data_science_pipelines","name":"Viewing existing pipelines","level":3,"index":7,"id":"viewing-existing-pipelines_ds-pipelines"},{"parentId":"working-with-data-science-pipelines_ds-pipelines","name":"Managing pipeline runs","level":2,"index":1,"id":"_managing_pipeline_runs"},{"parentId":"_managing_pipeline_runs","name":"Overview of pipeline runs","level":3,"index":0,"id":"overview-of-pipeline-runs_ds-pipelines"},{"parentId":"_managing_pipeline_runs","name":"Scheduling a pipeline run","level":3,"index":1,"id":"scheduling-a-pipeline-run_ds-pipelines"},{"parentId":"_managing_pipeline_runs","name":"Cloning a scheduled pipeline run","level":3,"index":2,"id":"cloning-a-scheduled-pipeline-run_ds-pipelines"},{"parentId":"_managing_pipeline_runs","name":"Stopping a triggered pipeline run","level":3,"index":3,"id":"stopping-a-triggered-pipeline-run_ds-pipelines"},{"parentId":"_managing_pipeline_runs","name":"Deleting a scheduled pipeline run","level":3,"index":4,"id":"deleting-a-scheduled-pipeline-run_ds-pipelines"},{"parentId":"_managing_pipeline_runs","name":"Deleting a triggered pipeline run","level":3,"index":5,"id":"deleting-a-triggered-pipeline-run_ds-pipelines"},{"parentId":"_managing_pipeline_runs","name":"Viewing scheduled pipeline runs","level":3,"index":6,"id":"viewing-scheduled-pipeline-runs_ds-pipelines"},{"parentId":"_managing_pipeline_runs","name":"Viewing triggered pipeline runs","level":3,"index":7,"id":"viewing-triggered-pipeline-runs_ds-pipelines"},{"parentId":"_managing_pipeline_runs","name":"Viewing the details of a pipeline run","level":3,"index":8,"id":"viewing-the-details-of-a-pipeline-run_ds-pipelines"},{"parentId":"working-with-data-science-pipelines_ds-pipelines","name":"Working with pipelines in JupyterLab","level":2,"index":2,"id":"_working_with_pipelines_in_jupyterlab"},{"parentId":"_working_with_pipelines_in_jupyterlab","name":"Overview of pipelines in JupyterLab","level":3,"index":0,"id":"overview-of-pipelines-in-jupyterlab_ds-pipelines"},{"parentId":"_working_with_pipelines_in_jupyterlab","name":"Accessing the pipeline editor","level":3,"index":1,"id":"accessing-the-pipeline-editor_ds-pipelines"},{"parentId":"_working_with_pipelines_in_jupyterlab","name":"Creating a runtime configuration","level":3,"index":2,"id":"creating-a-runtime-configuration_ds-pipelines"},{"parentId":"_working_with_pipelines_in_jupyterlab","name":"Updating a runtime configuration","level":3,"index":3,"id":"updating-a-runtime-configuration_ds-pipelines"},{"parentId":"_working_with_pipelines_in_jupyterlab","name":"Deleting a runtime configuration","level":3,"index":4,"id":"deleting-a-runtime-configuration_ds-pipelines"},{"parentId":"_working_with_pipelines_in_jupyterlab","name":"Duplicating a runtime configuration","level":3,"index":5,"id":"duplicating-a-runtime-configuration_ds-pipelines"},{"parentId":"_working_with_pipelines_in_jupyterlab","name":"Running a pipeline in JupyterLab","level":3,"index":6,"id":"running-a-pipeline-in-jupyterlab_ds-pipelines"},{"parentId":"_working_with_pipelines_in_jupyterlab","name":"Exporting a pipeline in JupyterLab","level":3,"index":7,"id":"exporting-a-pipeline-in-jupyterlab_ds-pipelines"},{"parentId":"working-with-data-science-pipelines_ds-pipelines","name":"Additional resources","level":2,"index":3,"id":"_additional_resources"},{"parentId":null,"name":"Model serving on Open Data Hub","level":1,"index":4,"id":"model-serving-on-openshift-data-science_model-serving"},{"parentId":"model-serving-on-openshift-data-science_model-serving","name":"Configuring model servers","level":2,"index":0,"id":"_configuring_model_servers_2"},{"parentId":"_configuring_model_servers_2","name":"Adding a custom model-serving runtime","level":3,"index":0,"id":"adding-a-custom-model-serving-runtime_model-serving"},{"parentId":"_configuring_model_servers_2","name":"Updating a model server","level":3,"index":1,"id":"updating-a-model-server_model-serving"},{"parentId":"_configuring_model_servers_2","name":"Deleting a model server","level":3,"index":2,"id":"deleting-a-model-server_model-serving"},{"parentId":"model-serving-on-openshift-data-science_model-serving","name":"Working with deployed models","level":2,"index":1,"id":"_working_with_deployed_models"},{"parentId":"_working_with_deployed_models","name":"Deploying a model in Open Data Hub","level":3,"index":0,"id":"deploying-a-model_model-serving"},{"parentId":"_working_with_deployed_models","name":"Viewing a deployed model","level":3,"index":1,"id":"viewing-a-deployed-model_model-serving"},{"parentId":"_working_with_deployed_models","name":"Updating the deployment properties of a deployed model","level":3,"index":2,"id":"updating-the-deployment-properties-of-a-deployed-model_model-serving"},{"parentId":"_working_with_deployed_models","name":"Deleting a deployed model","level":3,"index":3,"id":"deleting-a-deployed-model_model-serving"},{"parentId":null,"name":"Troubleshooting common problems in Jupyter for administrators","level":1,"index":5,"id":"troubleshooting-common-problems-in-jupyter-for-administrators_model-serving"},{"parentId":"troubleshooting-common-problems-in-jupyter-for-administrators_model-serving","name":"A user receives a <strong>404: Page not found</strong> error when logging in to Jupyter","level":2,"index":0,"id":"_a_user_receives_a_404_page_not_found_error_when_logging_in_to_jupyter"},{"parentId":"troubleshooting-common-problems-in-jupyter-for-administrators_model-serving","name":"A user&#8217;s notebook server does not start","level":2,"index":1,"id":"_a_users_notebook_server_does_not_start"},{"parentId":"troubleshooting-common-problems-in-jupyter-for-administrators_model-serving","name":"The user receives a <strong>database or disk is full</strong> error or a <strong>no space left on device</strong> error when they run notebook cells","level":2,"index":2,"id":"_the_user_receives_a_database_or_disk_is_full_error_or_a_no_space_left_on_device_error_when_they_run_notebook_cells"},{"parentId":null,"name":"Troubleshooting common problems in Jupyter for users","level":1,"index":6,"id":"troubleshooting-common-problems-in-jupyter-for-users_model-serving"},{"parentId":"troubleshooting-common-problems-in-jupyter-for-users_model-serving","name":"I see a <strong>403: Forbidden</strong> error when I log in to Jupyter","level":2,"index":0,"id":"_i_see_a_403_forbidden_error_when_i_log_in_to_jupyter"},{"parentId":"troubleshooting-common-problems-in-jupyter-for-users_model-serving","name":"My notebook server does not start","level":2,"index":1,"id":"_my_notebook_server_does_not_start"},{"parentId":"troubleshooting-common-problems-in-jupyter-for-users_model-serving","name":"I see a <strong>database or disk is full</strong> error or a <strong>no space left on device</strong> error when I run my notebook cells","level":2,"index":2,"id":"_i_see_a_database_or_disk_is_full_error_or_a_no_space_left_on_device_error_when_i_run_my_notebook_cells"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/_artifacts/document-attributes-global/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/collaborating-on-notebooks-using-git/"},"sections":[{"parentId":null,"name":"Uploading an existing notebook file from a Git repository using JupyterLab","level":1,"index":0,"id":"uploading-an-existing-notebook-file-from-a-git-repository-using-jupyterlab_git-collab"},{"parentId":null,"name":"Uploading an existing notebook file from a Git repository using the command line interface","level":1,"index":1,"id":"uploading-an-existing-notebook-file-from-a-git-repository-using-the-command-line-interface_git-collab"},{"parentId":null,"name":"Updating your project with changes from a remote Git repository","level":1,"index":2,"id":"updating-your-project-with-changes-from-a-remote-git-repository_git-collab"},{"parentId":null,"name":"Pushing project changes to a Git repository","level":1,"index":3,"id":"pushing-project-changes-to-a-git-repository_git-collab"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/creating-and-importing-notebooks/"},"sections":[{"parentId":null,"name":"Creating a new notebook","level":1,"index":0,"id":"creating-a-new-notebook_notebooks"},{"parentId":"creating-a-new-notebook_notebooks","name":"Notebook images for data scientists","level":2,"index":0,"id":"notebook-images-for-data-scientists_notebooks"},{"parentId":null,"name":"Uploading an existing notebook file from local storage","level":1,"index":1,"id":"uploading-an-existing-notebook-file-from-local-storage_notebooks"},{"parentId":null,"name":"Uploading an existing notebook file from a Git repository using JupyterLab","level":1,"index":2,"id":"uploading-an-existing-notebook-file-from-a-git-repository-using-jupyterlab_notebooks"},{"parentId":null,"name":"Uploading an existing notebook file from a Git repository using the command line interface","level":1,"index":3,"id":"uploading-an-existing-notebook-file-from-a-git-repository-using-the-command-line-interface_notebooks"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/model-serving/"},"sections":[{"parentId":null,"name":"Configuring model servers","level":1,"index":0,"id":"_configuring_model_servers"},{"parentId":"_configuring_model_servers","name":"Adding a custom model-serving runtime","level":2,"index":0,"id":"adding-a-custom-model-serving-runtime_model-serving"},{"parentId":"_configuring_model_servers","name":"Updating a model server","level":2,"index":1,"id":"updating-a-model-server_model-serving"},{"parentId":"_configuring_model_servers","name":"Deleting a model server","level":2,"index":2,"id":"deleting-a-model-server_model-serving"},{"parentId":null,"name":"Working with deployed models","level":1,"index":1,"id":"_working_with_deployed_models"},{"parentId":"_working_with_deployed_models","name":"Deploying a model in {productname-short}","level":2,"index":0,"id":"deploying-a-model_model-serving"},{"parentId":"_working_with_deployed_models","name":"Viewing a deployed model","level":2,"index":1,"id":"viewing-a-deployed-model_model-serving"},{"parentId":"_working_with_deployed_models","name":"Updating the deployment properties of a deployed model","level":2,"index":2,"id":"updating-the-deployment-properties-of-a-deployed-model_model-serving"},{"parentId":"_working_with_deployed_models","name":"Deleting a deployed model","level":2,"index":3,"id":"deleting-a-deployed-model_model-serving"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/support-requirements-and-limitations/"},"sections":[{"parentId":null,"name":"Supported browsers","level":1,"index":0,"id":"supported-browsers_requirements"},{"parentId":null,"name":"Supported services","level":1,"index":1,"id":"supported-services_requirements"},{"parentId":null,"name":"Supported packages","level":1,"index":2,"id":"supported-packages_requirements"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/working-on-data-science-projects/"},"sections":[{"parentId":null,"name":"Using data science projects","level":1,"index":0,"id":"_using_data_science_projects"},{"parentId":"_using_data_science_projects","name":"Creating a data science project","level":2,"index":0,"id":"creating-a-data-science-project_nb-server"},{"parentId":"_using_data_science_projects","name":"Updating a data science project","level":2,"index":1,"id":"updating-a-data-science-project_nb-server"},{"parentId":"_using_data_science_projects","name":"Deleting a data science project","level":2,"index":2,"id":"deleting-a-data-science-project_nb-server"},{"parentId":null,"name":"Using project workbenches","level":1,"index":1,"id":"_using_project_workbenches"},{"parentId":"_using_project_workbenches","name":"Creating a project workbench","level":2,"index":0,"id":"creating-a-project-workbench_nb-server"},{"parentId":"_using_project_workbenches","name":"Starting a workbench","level":2,"index":1,"id":"starting-a-workbench_nb-server"},{"parentId":"_using_project_workbenches","name":"Updating a project workbench","level":2,"index":2,"id":"updating-a-project-workbench_nb-server"},{"parentId":"_using_project_workbenches","name":"Deleting a workbench from a data science project","level":2,"index":3,"id":"deleting-a-workbench-from-a-data-science-project_nb-server"},{"parentId":null,"name":"Using data connections","level":1,"index":2,"id":"_using_data_connections"},{"parentId":"_using_data_connections","name":"Adding a data connection to your data science project","level":2,"index":0,"id":"adding-a-data-connection-to-your-data-science-project_nb-server"},{"parentId":"_using_data_connections","name":"Deleting a data connection","level":2,"index":1,"id":"deleting-a-data-connection_nb-server"},{"parentId":"_using_data_connections","name":"Updating a connected data source","level":2,"index":2,"id":"updating-a-connected-data-source_nb-server"},{"parentId":null,"name":"Configuring cluster storage","level":1,"index":3,"id":"_configuring_cluster_storage"},{"parentId":"_configuring_cluster_storage","name":"Adding cluster storage to your data science project","level":2,"index":0,"id":"adding-cluster-storage-to-your-data-science-project_nb-server"},{"parentId":"_configuring_cluster_storage","name":"Updating cluster storage","level":2,"index":1,"id":"updating-cluster-storage_nb-server"},{"parentId":"_configuring_cluster_storage","name":"Deleting cluster storage from a data science project","level":2,"index":2,"id":"deleting-cluster-storage-from-a-data-science-project_nb-server"},{"parentId":null,"name":"Configuring model servers","level":1,"index":4,"id":"_configuring_model_servers"},{"parentId":"_configuring_model_servers","name":"Adding a custom model-serving runtime","level":2,"index":0,"id":"adding-a-custom-model-serving-runtime_nb-server"},{"parentId":"_configuring_model_servers","name":"Updating a model server","level":2,"index":1,"id":"updating-a-model-server_nb-server"},{"parentId":"_configuring_model_servers","name":"Deleting a model server","level":2,"index":2,"id":"deleting-a-model-server_nb-server"},{"parentId":null,"name":"Configuring access to data science projects","level":1,"index":5,"id":"_configuring_access_to_data_science_projects"},{"parentId":"_configuring_access_to_data_science_projects","name":"Configuring access to data science projects","level":2,"index":0,"id":"configuring-access-to-data-science-projects_nb-server"},{"parentId":"_configuring_access_to_data_science_projects","name":"Sharing access to a data science project","level":2,"index":1,"id":"sharing-access-to-a-data-science-project_nb-server"},{"parentId":"_configuring_access_to_data_science_projects","name":"Updating access to a data science project","level":2,"index":2,"id":"updating-access-to-a-data-science-project_nb-server"},{"parentId":"_configuring_access_to_data_science_projects","name":"Removing access to a data science project","level":2,"index":3,"id":"removing-access-to-a-data-science-project_nb-server"},{"parentId":null,"name":"Viewing Python packages installed on your notebook server","level":1,"index":6,"id":"viewing-python-packages-installed-on-your-notebook-server_nb-server"},{"parentId":null,"name":"Installing Python packages on your notebook server","level":1,"index":7,"id":"installing-python-packages-on-your-notebook-server_nb-server"},{"parentId":null,"name":"Updating notebook server settings by restarting your server","level":1,"index":8,"id":"updating-notebook-server-settings-by-restarting-your-server_nb-server"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/working-with-data-science-pipelines/"},"sections":[{"parentId":null,"name":"Managing data science pipelines","level":1,"index":0,"id":"_managing_data_science_pipelines"},{"parentId":"_managing_data_science_pipelines","name":"Configuring a pipeline server","level":2,"index":0,"id":"configuring-a-pipeline-server_ds-pipelines"},{"parentId":"_managing_data_science_pipelines","name":"Defining a pipeline","level":2,"index":1,"id":"defining-a-pipeline_ds-pipelines"},{"parentId":"_managing_data_science_pipelines","name":"Importing a data science pipeline","level":2,"index":2,"id":"importing-a-data-science-pipeline_ds-pipelines"},{"parentId":"_managing_data_science_pipelines","name":"Downloading a data science pipeline","level":2,"index":3,"id":"downloading-a-data-science-pipeline_ds-pipelines"},{"parentId":"_managing_data_science_pipelines","name":"Deleting a data science pipeline","level":2,"index":4,"id":"deleting-a-data-science-pipeline_ds-pipelines"},{"parentId":"_managing_data_science_pipelines","name":"Deleting a pipeline server","level":2,"index":5,"id":"deleting-a-pipeline-server_ds-pipelines"},{"parentId":"_managing_data_science_pipelines","name":"Viewing the details of a pipeline server","level":2,"index":6,"id":"viewing-the-details-of-a-pipeline-server_ds-pipelines"},{"parentId":"_managing_data_science_pipelines","name":"Viewing existing pipelines","level":2,"index":7,"id":"viewing-existing-pipelines_ds-pipelines"},{"parentId":null,"name":"Managing pipeline runs","level":1,"index":1,"id":"_managing_pipeline_runs"},{"parentId":"_managing_pipeline_runs","name":"Overview of pipeline runs","level":2,"index":0,"id":"overview-of-pipeline-runs_ds-pipelines"},{"parentId":"_managing_pipeline_runs","name":"Scheduling a pipeline run","level":2,"index":1,"id":"scheduling-a-pipeline-run_ds-pipelines"},{"parentId":"_managing_pipeline_runs","name":"Cloning a scheduled pipeline run","level":2,"index":2,"id":"cloning-a-scheduled-pipeline-run_ds-pipelines"},{"parentId":"_managing_pipeline_runs","name":"Stopping a triggered pipeline run","level":2,"index":3,"id":"stopping-a-triggered-pipeline-run_ds-pipelines"},{"parentId":"_managing_pipeline_runs","name":"Deleting a scheduled pipeline run","level":2,"index":4,"id":"deleting-a-scheduled-pipeline-run_ds-pipelines"},{"parentId":"_managing_pipeline_runs","name":"Deleting a triggered pipeline run","level":2,"index":5,"id":"deleting-a-triggered-pipeline-run_ds-pipelines"},{"parentId":"_managing_pipeline_runs","name":"Viewing scheduled pipeline runs","level":2,"index":6,"id":"viewing-scheduled-pipeline-runs_ds-pipelines"},{"parentId":"_managing_pipeline_runs","name":"Viewing triggered pipeline runs","level":2,"index":7,"id":"viewing-triggered-pipeline-runs_ds-pipelines"},{"parentId":"_managing_pipeline_runs","name":"Viewing the details of a pipeline run","level":2,"index":8,"id":"viewing-the-details-of-a-pipeline-run_ds-pipelines"},{"parentId":null,"name":"Working with pipelines in JupyterLab","level":1,"index":2,"id":"_working_with_pipelines_in_jupyterlab"},{"parentId":"_working_with_pipelines_in_jupyterlab","name":"Overview of pipelines in JupyterLab","level":2,"index":0,"id":"overview-of-pipelines-in-jupyterlab_ds-pipelines"},{"parentId":"_working_with_pipelines_in_jupyterlab","name":"Accessing the pipeline editor","level":2,"index":1,"id":"accessing-the-pipeline-editor_ds-pipelines"},{"parentId":"_working_with_pipelines_in_jupyterlab","name":"Creating a runtime configuration","level":2,"index":2,"id":"creating-a-runtime-configuration_ds-pipelines"},{"parentId":"_working_with_pipelines_in_jupyterlab","name":"Updating a runtime configuration","level":2,"index":3,"id":"updating-a-runtime-configuration_ds-pipelines"},{"parentId":"_working_with_pipelines_in_jupyterlab","name":"Deleting a runtime configuration","level":2,"index":4,"id":"deleting-a-runtime-configuration_ds-pipelines"},{"parentId":"_working_with_pipelines_in_jupyterlab","name":"Duplicating a runtime configuration","level":2,"index":5,"id":"duplicating-a-runtime-configuration_ds-pipelines"},{"parentId":"_working_with_pipelines_in_jupyterlab","name":"Running a pipeline in JupyterLab","level":2,"index":6,"id":"running-a-pipeline-in-jupyterlab_ds-pipelines"},{"parentId":"_working_with_pipelines_in_jupyterlab","name":"Exporting a pipeline in JupyterLab","level":2,"index":7,"id":"exporting-a-pipeline-in-jupyterlab_ds-pipelines"},{"parentId":null,"name":"Additional resources","level":1,"index":3,"id":"_additional_resources"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/accessing-the-pipeline-editor/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/accessing-tutorials/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/adding-a-custom-model-serving-runtime/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/adding-a-data-connection-to-your-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/adding-cluster-storage-to-your-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/cloning-a-scheduled-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/common-questions/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-a-model-server-for-your-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-a-pipeline-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-access-to-data-science-projects/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-a-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-a-new-notebook/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-a-project-workbench/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-a-runtime-configuration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/defining-a-pipeline/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-a-data-connection/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-a-data-science-pipeline/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-a-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-a-deployed-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-a-model-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-a-pipeline-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-a-runtime-configuration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-a-scheduled-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-a-triggered-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-a-workbench-from-a-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-cluster-storage-from-a-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deploying-a-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/disabling-applications-connected-to-open-data-hub/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/downloading-a-data-science-pipeline/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/duplicating-a-runtime-configuration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/enabling-services-connected-to-open-data-hub/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/exporting-a-pipeline-in-jupyterlab/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/importing-a-data-science-pipeline/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/installing-python-packages-on-your-notebook-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/launching-jupyter-and-starting-a-notebook-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/logging-in-to-open-data-hub/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/notebook-images-for-data-scientists/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/notifications-in-open-data-hub/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/options-for-notebook-server-environments/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/overview-of-pipeline-runs/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/overview-of-pipelines-in-jupyterlab/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/pushing-project-changes-to-a-git-repository/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/removing-access-to-a-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/removing-disabled-applications-from-open-data-hub/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/running-a-pipeline-in-jupyterlab/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/scheduling-a-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/sharing-access-to-a-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/starting-a-workbench/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/stopping-a-triggered-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/supported-browsers/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/supported-packages/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/supported-services/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/the-open-data-hub-user-interface/"},"sections":[{"parentId":null,"name":"Global navigation","level":1,"index":0,"id":"_global_navigation"},{"parentId":null,"name":"Side navigation","level":1,"index":1,"id":"_side_navigation"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/troubleshooting-common-problems-in-jupyter-for-administrators/"},"sections":[{"parentId":null,"name":"A user receives a <strong>404: Page not found</strong> error when logging in to Jupyter","level":1,"index":0,"id":"_a_user_receives_a_404_page_not_found_error_when_logging_in_to_jupyter"},{"parentId":null,"name":"A user&#8217;s notebook server does not start","level":1,"index":1,"id":"_a_users_notebook_server_does_not_start"},{"parentId":null,"name":"The user receives a <strong>database or disk is full</strong> error or a <strong>no space left on device</strong> error when they run notebook cells","level":1,"index":2,"id":"_the_user_receives_a_database_or_disk_is_full_error_or_a_no_space_left_on_device_error_when_they_run_notebook_cells"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/troubleshooting-common-problems-in-jupyter-for-users/"},"sections":[{"parentId":null,"name":"I see a <strong>403: Forbidden</strong> error when I log in to Jupyter","level":1,"index":0,"id":"_i_see_a_403_forbidden_error_when_i_log_in_to_jupyter"},{"parentId":null,"name":"My notebook server does not start","level":1,"index":1,"id":"_my_notebook_server_does_not_start"},{"parentId":null,"name":"I see a <strong>database or disk is full</strong> error or a <strong>no space left on device</strong> error when I run my notebook cells","level":1,"index":2,"id":"_i_see_a_database_or_disk_is_full_error_or_a_no_space_left_on_device_error_when_i_run_my_notebook_cells"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/tutorials-for-data-scientists/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/updating-a-connected-data-source/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/updating-a-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/updating-a-model-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/updating-a-project-workbench/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/updating-a-runtime-configuration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/updating-access-to-a-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/updating-cluster-storage/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/updating-notebook-server-settings-by-restarting-your-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/updating-the-deployment-properties-of-a-deployed-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/updating-your-project-with-changes-from-a-remote-git-repository/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/uploading-an-existing-notebook-file-from-a-git-repository-using-jupyterlab/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/uploading-an-existing-notebook-file-from-a-git-repository-using-the-command-line-interface/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/uploading-an-existing-notebook-file-from-local-storage/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-a-deployed-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-existing-pipelines/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-python-packages-installed-on-your-notebook-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-scheduled-pipeline-runs/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-the-details-of-a-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-the-details-of-a-pipeline-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-the-performance-metrics-of-a-deployed-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-triggered-pipeline-runs/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/accessing-the-pipeline-editor/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/accessing-tutorials/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/adding-a-custom-model-serving-runtime/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/adding-a-data-connection-to-your-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/adding-cluster-storage-to-your-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/cloning-a-scheduled-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/common-questions/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-a-model-server-for-your-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-a-pipeline-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-access-to-data-science-projects/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-a-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-a-new-notebook/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-a-project-workbench/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-a-runtime-configuration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/defining-a-pipeline/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-a-data-connection/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-a-data-science-pipeline/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-a-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-a-deployed-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-a-model-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-a-pipeline-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-a-runtime-configuration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-a-scheduled-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-a-triggered-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-a-workbench-from-a-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-cluster-storage-from-a-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deploying-a-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/disabling-applications-connected-to-open-data-hub/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/downloading-a-data-science-pipeline/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/duplicating-a-runtime-configuration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/enabling-services-connected-to-open-data-hub/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/exporting-a-pipeline-in-jupyterlab/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/importing-a-data-science-pipeline/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/installing-python-packages-on-your-notebook-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/launching-jupyter-and-starting-a-notebook-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/logging-in-to-open-data-hub/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/notebook-images-for-data-scientists/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/notifications-in-open-data-hub/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/options-for-notebook-server-environments/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/overview-of-pipeline-runs/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/overview-of-pipelines-in-jupyterlab/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/pushing-project-changes-to-a-git-repository/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/removing-access-to-a-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/removing-disabled-applications-from-open-data-hub/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/running-a-pipeline-in-jupyterlab/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/scheduling-a-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/sharing-access-to-a-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/starting-a-workbench/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/stopping-a-triggered-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/supported-browsers/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/supported-packages/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/supported-services/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/the-open-data-hub-user-interface/"},"sections":[{"parentId":null,"name":"Global navigation","level":1,"index":0,"id":"_global_navigation"},{"parentId":null,"name":"Side navigation","level":1,"index":1,"id":"_side_navigation"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/troubleshooting-common-problems-in-jupyter-for-administrators/"},"sections":[{"parentId":null,"name":"A user receives a <strong>404: Page not found</strong> error when logging in to Jupyter","level":1,"index":0,"id":"_a_user_receives_a_404_page_not_found_error_when_logging_in_to_jupyter"},{"parentId":null,"name":"A user&#8217;s notebook server does not start","level":1,"index":1,"id":"_a_users_notebook_server_does_not_start"},{"parentId":null,"name":"The user receives a <strong>database or disk is full</strong> error or a <strong>no space left on device</strong> error when they run notebook cells","level":1,"index":2,"id":"_the_user_receives_a_database_or_disk_is_full_error_or_a_no_space_left_on_device_error_when_they_run_notebook_cells"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/troubleshooting-common-problems-in-jupyter-for-users/"},"sections":[{"parentId":null,"name":"I see a <strong>403: Forbidden</strong> error when I log in to Jupyter","level":1,"index":0,"id":"_i_see_a_403_forbidden_error_when_i_log_in_to_jupyter"},{"parentId":null,"name":"My notebook server does not start","level":1,"index":1,"id":"_my_notebook_server_does_not_start"},{"parentId":null,"name":"I see a <strong>database or disk is full</strong> error or a <strong>no space left on device</strong> error when I run my notebook cells","level":1,"index":2,"id":"_i_see_a_database_or_disk_is_full_error_or_a_no_space_left_on_device_error_when_i_run_my_notebook_cells"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/tutorials-for-data-scientists/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/updating-a-connected-data-source/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/updating-a-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/updating-a-model-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/updating-a-project-workbench/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/updating-access-to-a-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/updating-cluster-storage/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/updating-notebook-server-settings-by-restarting-your-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/updating-the-deployment-properties-of-a-deployed-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/updating-your-project-with-changes-from-a-remote-git-repository/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/uploading-an-existing-notebook-file-from-a-git-repository-using-jupyterlab/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/uploading-an-existing-notebook-file-from-a-git-repository-using-the-command-line-interface/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/uploading-an-existing-notebook-file-from-local-storage/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-a-deployed-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-existing-pipelines/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-python-packages-installed-on-your-notebook-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-scheduled-pipeline-runs/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-the-details-of-a-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-the-details-of-a-pipeline-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-the-performance-metrics-of-a-deployed-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-triggered-pipeline-runs/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/updating-a-runtime-configuration/"},"sections":null}}}]},"asciidoc":{"html":"<div id=\"preamble\">\n<div class=\"sectionbody\">\n<div class=\"paragraph _abstract\">\n<p>As a data scientist, you can enhance your data science projects on {productname-short} by building portable machine learning (ML) workflows with data science pipelines, using Docker containers. This enables you to standardize and automate machine learning workflows to enable you to develop and deploy your data science models.</p>\n</div>\n<div class=\"paragraph\">\n<p>For example, the steps in a machine learning workflow might include items such as data extraction, data processing, feature extraction, model training, model validation, and model serving. Automating these activities enables your organization to develop a continuous process of retraining and updating a model based on newly received data. This can help address challenges related to building an integrated machine learning deployment and continuously operating it in production.</p>\n</div>\n<div class=\"paragraph\">\n<p>You can also use the Elyra JupyterLab extension to create and run data science pipelines within JupyterLab. For more information, see <a href=\"{odhdocshome}{default-format-url}/working_on_data_science_projects/working-with-data-science-pipelines_ds-pipelines#working-with-pipelines-in-jupyterlab\">Working with pipelines in JupyterLab</a>.</p>\n</div>\n<div class=\"paragraph\">\n<p>A data science pipeline in {productname-short} consists of the following components:</p>\n</div>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>Pipeline server: A server that is attached to your data science project and hosts your data science pipeline.</p>\n</li>\n<li>\n<p>Pipeline: A pipeline defines the configuration of your machine learning workflow and the relationship between each component in the workflow.</p>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>Pipeline code: A definition of your pipeline in a Tekton-formatted YAML file.</p>\n</li>\n<li>\n<p>Pipeline graph: A graphical illustration of the steps executed in a pipeline run and the relationship between them.</p>\n</li>\n</ul>\n</div>\n</li>\n<li>\n<p>Pipeline run: An execution of your pipeline.</p>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>Triggered run: A previously executed pipeline run.</p>\n</li>\n<li>\n<p>Scheduled run: A pipeline run scheduled to execute at least once.</p>\n</li>\n</ul>\n</div>\n</li>\n</ul>\n</div>\n<div class=\"paragraph\">\n<p>This feature is based on Kubeflow Pipelines v1. Use the Kubeflow Pipelines SDK to build your data science pipeline in Python code. After you have built your pipeline, compile it into Tekton-formatted YAML code using kfp-tekton SDK (version 1.5.x only). The {productname-short} user interface enables you to track and manage pipelines and pipeline runs.</p>\n</div>\n<div class=\"paragraph\">\n<p>Before you can use data science pipelines, you must install the Data Science Pipelines operator as described in <a href=\"https://github.com/opendatahub-io/data-science-pipelines-operator\">Data Science Pipelines Operator</a>.</p>\n</div>\n<div class=\"paragraph\">\n<p>You can store your pipeline artifacts in an Amazon Web Services (AWS) Simple Storage Service (S3) bucket so that you do not consume local storage. To do this, you must first configure write access to your S3 bucket on your AWS account.</p>\n</div>\n</div>\n</div>\n<div class=\"sect1\">\n<h2 id=\"_managing_data_science_pipelines\">Managing data science pipelines</h2>\n<div class=\"sectionbody\">\n<div class=\"sect2\">\n<h3 id=\"configuring-a-pipeline-server_ds-pipelines\">Configuring a pipeline server</h3>\n<div class=\"paragraph _abstract\">\n<p>Before you can successfully create a pipeline in {productname-short}, you must configure a pipeline server. This includes configuring where your pipeline artifacts and data are stored.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have installed the OpenShift Pipelines operator.</p>\n</li>\n<li>\n<p>You have logged in to {productname-long}.</p>\n</li>\n<li>\n<p>If you are using specialized {productname-short} groups, you are part of the user group or admin group (for example, <code>odh-users</code> or <code>odh-admins</code>) in OpenShift.</p>\n</li>\n<li>\n<p>You have created a data science project that you can add a pipeline server to.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From the {productname-short} dashboard, click <strong>Data Science Projects</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Data science projects</strong> page opens.</p>\n</div>\n</li>\n<li>\n<p>Click the name of the project that you want to configure a pipeline server for.</p>\n<div class=\"paragraph\">\n<p>A project details page opens.</p>\n</div>\n</li>\n<li>\n<p>In the <strong>Pipelines</strong> section, click <strong>Create a pipeline server</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Configure pipeline server</strong> dialog appears.</p>\n</div>\n</li>\n<li>\n<p>In the <strong>Object storage connection</strong> section, to specify the S3-compatible data connection to store your pipeline artifacts, select one of the following sets of actions:</p>\n<div class=\"admonitionblock note\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Note</div>\n</td>\n<td class=\"content\">\n<div class=\"paragraph\">\n<p>After the pipeline server is created, the <code>/metadata</code> and <code>/artifacts</code> folders are automatically created in the default <code>root</code> folder. Therefore, you are not required to specify any storage directories when configuring a data connection for your pipeline server.</p>\n</div>\n</td>\n</tr>\n</table>\n</div>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>Select <strong>Existing data connection</strong> to use a data connection that you previously defined. If you selected this option, from the <strong>Name</strong> list, select the name of the relevant data connection and skip to step 6.</p>\n</li>\n<li>\n<p>Select <strong>Create new data connection</strong> to add a new data connection that your pipeline server can access.</p>\n</li>\n</ul>\n</div>\n</li>\n<li>\n<p>If you selected <strong>Create new data connection</strong>, perform the following steps:</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>In the <strong>Name</strong> field, enter a name for the data connection.</p>\n</li>\n<li>\n<p>In the <strong>AWS_ACCESS_KEY_ID</strong> field, enter your access key ID for Amazon Web Services.</p>\n</li>\n<li>\n<p>In the <strong>AWS_SECRET_ACCESS_KEY_ID</strong> field, enter your secret access key for the account you specified.</p>\n</li>\n<li>\n<p>Optional: In the <strong>AWS_S3_ENDPOINT</strong> field, enter the endpoint of your AWS S3 storage.</p>\n</li>\n<li>\n<p>Optional: In the <strong>AWS_DEFAULT_REGION</strong> field, enter the default region of your AWS account.</p>\n</li>\n<li>\n<p>In the <strong>AWS_S3_BUCKET</strong> field, enter the name of the AWS S3 bucket.</p>\n<div class=\"admonitionblock important\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Important</div>\n</td>\n<td class=\"content\">\n<div class=\"paragraph\">\n<p>If you are creating a new data connection, in addition to the other designated mandatory fields, the <strong>AWS_S3_BUCKET</strong> field is mandatory. If you specify incorrect data connection settings, you cannot update these settings on the same pipeline server. Therefore, you must delete the pipeline server and configure another one.</p>\n</div>\n</td>\n</tr>\n</table>\n</div>\n</li>\n</ol>\n</div>\n</li>\n<li>\n<p>In the <strong>Database</strong> section, click <strong>Show advanced database options</strong> to specify the database to store your pipeline data and select one of the following sets of actions:</p>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>Select <strong>Use default database stored on your cluster</strong> to deploy a MariaDB database in your project.</p>\n</li>\n<li>\n<p>Select <strong>Connect to external MySQL database</strong> to add a new connection to an external database that your pipeline server can access.</p>\n<div class=\"olist lowerroman\">\n<ol class=\"lowerroman\" type=\"i\">\n<li>\n<p>In the <strong>Host</strong> field, enter the database&#8217;s host name.</p>\n</li>\n<li>\n<p>In the <strong>Port</strong> field, enter the database&#8217;s port.</p>\n</li>\n<li>\n<p>In the <strong>Username</strong> field, enter the default user name that is connected to the database.</p>\n</li>\n<li>\n<p>In the <strong>Password</strong> field, enter the password for the default user account.</p>\n</li>\n<li>\n<p>In the <strong>Database</strong> field, enter the database name.</p>\n</li>\n</ol>\n</div>\n</li>\n</ul>\n</div>\n</li>\n<li>\n<p>Click <strong>Configure</strong>.</p>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>The pipeline server that you configured is displayed in the <strong>Pipelines</strong> section on the project details page.</p>\n</li>\n<li>\n<p>The <strong>Import pipeline</strong> button is available in the <strong>Pipelines</strong> section on the project details page.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"defining-a-pipeline_ds-pipelines\">Defining a pipeline</h3>\n<div class=\"paragraph _abstract\">\n<p>The Kubeflow Pipelines SDK enables you to define end-to-end machine learning and data pipelines. Use the Kubeflow Pipelines SDK to build your data science pipeline in Python code. After you have built your pipeline, compile it into Tekton-formatted YAML code using kfp-tekton SDK (version 1.5.x only). After defining the pipeline, you can import the YAML file to the {productname-short} dashboard to enable you to configure its execution settings. For more information about installing and using Kubeflow Pipelines SDK for Tetkon, see <a href=\"https://kubeflow.org/docs/components/pipelines/v1/sdk/pipelines-with-tekton/\">Kubeflow Pipelines SDK for Tekton</a>.</p>\n</div>\n<div class=\"paragraph\">\n<p>You can also use the Elyra JupyterLab extension to create and run data science pipelines within JupyterLab. For more information on the Elyra JupyterLab extension, see <a href=\"https://elyra.readthedocs.io/en/stable/getting_started/overview.html\">Elyra Documentation</a>.</p>\n</div>\n<div class=\"ulist _additional-resources\">\n<div class=\"title\">Additional resources</div>\n<ul>\n<li>\n<p><a href=\"https://github.com/kubeflow/kfp-tekton/tree/master/sdk\">Kubeflow Pipelines SDK for Tekton</a></p>\n</li>\n<li>\n<p><a href=\"https://github.com/kubeflow/kfp-tekton/tree/master/samples\">KFP Tekton samples and compiler samples</a></p>\n</li>\n<li>\n<p><a href=\"https://www.kubeflow.org/docs/components/pipelines/v1/\">Kubeflow Pipelines v1 Documentation</a></p>\n</li>\n<li>\n<p><a href=\"https://elyra.readthedocs.io/en/stable/getting_started/overview.html\">Elyra Documentation</a></p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"importing-a-data-science-pipeline_ds-pipelines\">Importing a data science pipeline</h3>\n<div class=\"paragraph _abstract\">\n<p>To help you begin working with data science pipelines in {productname-short}, you can import a YAML file containing your pipeline&#8217;s code to an active pipeline server. This file contains a Kubeflow pipeline compiled with the Tekton compiler. After you have imported the pipeline to a pipeline server, you can execute the pipeline by creating a pipeline run.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have installed the OpenShift Pipelines operator.</p>\n</li>\n<li>\n<p>You have logged in to {productname-long}.</p>\n</li>\n<li>\n<p>If you are using specialized {productname-short} groups, you are part of the user group or admin group (for example, <code>odh-users</code> or <code>odh-admins</code>) in OpenShift.</p>\n</li>\n<li>\n<p>You have previously created a data science project that is available and contains a configured pipeline server.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From the {productname-short} dashboard, click <strong>Data Science Pipelines</strong> &#8594; <strong>Pipelines</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Pipelines</strong> page opens.</p>\n</div>\n</li>\n<li>\n<p>From the <strong>Project</strong> list, select the project that you want to import a pipeline to.</p>\n</li>\n<li>\n<p>Click <strong>Import pipeline</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Import pipeline</strong> dialog opens.</p>\n</div>\n</li>\n<li>\n<p>Enter the details for the pipeline that you are importing.</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>In the <strong>Pipeline name</strong> field, enter a name for the pipeline that you are importing.</p>\n</li>\n<li>\n<p>In the <strong>Pipeline description</strong> field, enter a description for the pipeline that you are importing.</p>\n</li>\n<li>\n<p>Click <strong>Upload</strong>. Alternatively, drag the file from your local machine&#8217;s file system and drop it in the designated area in the <strong>Import pipeline</strong> dialog.</p>\n<div class=\"paragraph\">\n<p>A file browser opens.</p>\n</div>\n</li>\n<li>\n<p>Navigate to the file containing the pipeline code and click <strong>Select</strong>.</p>\n</li>\n<li>\n<p>Click <strong>Import pipeline</strong>.</p>\n</li>\n</ol>\n</div>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>The pipeline that you imported is displayed on the <strong>Pipelines</strong> page.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"downloading-a-data-science-pipeline_ds-pipelines\">Downloading a data science pipeline</h3>\n<div class=\"paragraph _abstract\">\n<p>To make further changes to a data science pipeline that you previously uploaded to {productname-short}, you can download the pipeline&#8217;s code from the user interface.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have installed the OpenShift Pipelines operator.</p>\n</li>\n<li>\n<p>You have logged in to {productname-long}.</p>\n</li>\n<li>\n<p>If you are using specialized {productname-short} groups, you are part of the user group or admin group (for example, <code>odh-users</code> or <code>odh-admins</code>) in OpenShift.</p>\n</li>\n<li>\n<p>You have previously created a data science project that is available and contains a configured pipeline server.</p>\n</li>\n<li>\n<p>You have created and imported a pipeline to an active pipeline server that is available to download.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From the {productname-short} dashboard, click <strong>Data Science Pipelines</strong> &#8594; <strong>Pipelines</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Pipelines</strong> page opens.</p>\n</div>\n</li>\n<li>\n<p>From the <strong>Project</strong> list, select the project whose pipeline that you want to download.</p>\n</li>\n<li>\n<p>In the <strong>Pipeline name</strong> column, click the name of the pipeline that you want to download.</p>\n<div class=\"paragraph\">\n<p>The <strong>Pipeline details</strong> page opens displaying the <strong>Graph</strong> tab.</p>\n</div>\n</li>\n<li>\n<p>Click the <strong>YAML</strong> tab.</p>\n<div class=\"paragraph\">\n<p>The page reloads to display an embedded YAML editor showing the pipeline code.</p>\n</div>\n</li>\n<li>\n<p>Click the <strong>Download</strong> button (<span class=\"image\"><img src=\"/static/docs/images/rhods-download-icon.png\" alt=\"rhods download icon\"></span>) to download the YAML file containing your pipeline&#8217;s code to your local machine.</p>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>The pipeline code is downloaded to your browser&#8217;s default directory for downloaded files.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"deleting-a-data-science-pipeline_ds-pipelines\">Deleting a data science pipeline</h3>\n<div class=\"paragraph _abstract\">\n<p>You can delete data science pipelines so that they do not appear on the {productname-short} <strong>Pipelines</strong> page.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have installed the OpenShift Pipelines operator.</p>\n</li>\n<li>\n<p>You have logged in to {productname-long}.</p>\n</li>\n<li>\n<p>If you are using specialized {productname-short} groups, you are part of the user group or admin group (for example, <code>odh-users</code> or <code>odh-admins</code>) in OpenShift.</p>\n</li>\n<li>\n<p>There are active pipelines available on the <strong>Pipelines</strong> page.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From the {productname-short} dashboard, click <strong>Data Science Pipelines</strong> &#8594; <strong>Pipelines</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Pipelines</strong> page opens.</p>\n</div>\n</li>\n<li>\n<p>From the <strong>Project</strong> list, select the project that contains the pipeline that you want to delete.</p>\n</li>\n<li>\n<p>Click the action menu (<strong>&#8942;</strong>) beside the pipeline that you want to delete and click <strong>Delete pipeline</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Delete pipeline</strong> dialog opens.</p>\n</div>\n</li>\n<li>\n<p>Enter the pipeline name in the text field to confirm that you intend to delete it.</p>\n</li>\n<li>\n<p>Click <strong>Delete pipeline</strong>.</p>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>The data science pipeline that you deleted is no longer displayed on the <strong>Pipelines</strong> page.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"deleting-a-pipeline-server_ds-pipelines\">Deleting a pipeline server</h3>\n<div class=\"paragraph _abstract\">\n<p>After you have finished running your data science pipelines, you can delete the pipeline server. Deleting a pipeline server automatically deletes all of its associated pipelines and runs. If your pipeline data is stored in a database, the database is also deleted along with its meta-data. In addition, after deleting a pipeline server, you cannot create new pipelines or pipeline runs until you create another pipeline server.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to {productname-long}.</p>\n</li>\n<li>\n<p>If you are using specialized {productname-short} groups, you are part of the user group or admin group (for example, <code>odh-users</code> or <code>odh-admins</code>) in OpenShift.</p>\n</li>\n<li>\n<p>You have previously created a data science project that is available and contains a pipeline server.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From the {productname-short} dashboard, click <strong>Data Science Pipelines</strong> &#8594; <strong>Pipelines</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Pipelines</strong> page opens.</p>\n</div>\n</li>\n<li>\n<p>From the <strong>Project</strong> list, select the project whose pipeline server you want to delete.</p>\n</li>\n<li>\n<p>From the <strong>Pipeline server actions</strong> list, select <strong>Delete pipeline server</strong>.\nThe <strong>Delete pipeline server</strong> dialog opens.</p>\n</li>\n<li>\n<p>Enter the pipeline server&#8217;s name in the text field to confirm that you intend to delete it.</p>\n</li>\n<li>\n<p>Click <strong>Delete</strong>.</p>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>Pipelines previously assigned to the deleted pipeline server are no longer displayed on the <strong>Pipelines</strong> page for the relevant data science project.</p>\n</li>\n<li>\n<p>Pipeline runs previously assigned to the deleted pipeline server are no longer displayed on the <strong>Runs</strong> page for the relevant data science project.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"viewing-the-details-of-a-pipeline-server_ds-pipelines\">Viewing the details of a pipeline server</h3>\n<div class=\"paragraph _abstract\">\n<p>You can view the details of pipeline servers configured in {productname-short}, such as the pipeline&#8217;s data connection details and where its data is stored.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have installed the OpenShift Pipelines operator.</p>\n</li>\n<li>\n<p>You have logged in to {productname-long}.</p>\n</li>\n<li>\n<p>You have previously created a data science project that contains an active and available pipeline server.</p>\n</li>\n<li>\n<p>If you are using specialized {productname-short} groups, you are part of the user group or admin group (for example, <code>odh-users</code> or <code>odh-admins</code>) in OpenShift.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From the {productname-short} dashboard, click <strong>Data Science Pipelines</strong> &#8594; <strong>Pipelines</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Pipelines</strong> page opens.</p>\n</div>\n</li>\n<li>\n<p>From the <strong>Project</strong> list, select the project whose pipeline server you want to view.</p>\n</li>\n<li>\n<p>From the <strong>Pipeline server actions</strong> list, select <strong>View pipeline server configuration</strong>.</p>\n</li>\n<li>\n<p>When you have finished inspecting the pipeline server&#8217;s details, click <strong>Done</strong>.</p>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>You can view the relevant pipeline server&#8217;s details in the <strong>View pipeline server</strong> dialog.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"viewing-existing-pipelines_ds-pipelines\">Viewing existing pipelines</h3>\n<div class=\"paragraph _abstract\">\n<p>You can view the details of pipelines that you have imported to {productname-long}, such as the pipeline&#8217;s last run, when it was created, and the pipeline&#8217;s executed runs.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have installed the OpenShift Pipelines operator.</p>\n</li>\n<li>\n<p>You have logged in to {productname-long}.</p>\n</li>\n<li>\n<p>If you are using specialized {productname-short} groups, you are part of the user group or admin group (for example, <code>odh-users</code> or <code>odh-admins</code>) in OpenShift.</p>\n</li>\n<li>\n<p>You have previously created a data science project that is available and contains a pipeline server.</p>\n</li>\n<li>\n<p>You have imported a pipeline to an active and available pipeline server.</p>\n</li>\n<li>\n<p>The pipeline you imported is available, or there are other previously imported pipelines available to view.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From the {productname-short} dashboard, click <strong>Data Science Pipelines</strong> &#8594; <strong>Pipelines</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Pipelines</strong> page opens.</p>\n</div>\n</li>\n<li>\n<p>From the <strong>Project</strong> list, select the relevant project whose pipelines you want to view.</p>\n</li>\n<li>\n<p>Study the pipelines on the list.</p>\n</li>\n<li>\n<p>Optional: Click <strong>Expand</strong> (<span class=\"image\"><img src=\"/static/docs/images/rhods-expand-icon.png\" alt=\"rhods expand icon\"></span>) on the relevant row to view the pipeline&#8217;s executed runs. If the pipeline does not contain any runs, click <strong>Create run</strong> to create one.</p>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>A list of previously created data science pipelines is displayed on the <strong>Pipelines</strong> page.</p>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>\n<div class=\"sect1\">\n<h2 id=\"_managing_pipeline_runs\">Managing pipeline runs</h2>\n<div class=\"sectionbody\">\n<div class=\"sect2\">\n<h3 id=\"overview-of-pipeline-runs_ds-pipelines\">Overview of pipeline runs</h3>\n<div class=\"paragraph _abstract\">\n<p>A pipeline run is a single execution of a data science pipeline. As data scientist, you can use {productname-short} to define, manage, and track executions of a data science pipeline. You can view a record of your data science project&#8217;s previously executed and scheduled runs from the <strong>Runs</strong> page in the {productname-short} user interface.</p>\n</div>\n<div class=\"paragraph\">\n<p>Runs are intended for portability. Therefore, you can clone your pipeline runs to reproduce and scale them accordingly, or delete them when you longer require them. You can configure a run to execute only once immediately after creation or on a recurring basis. Recurring runs consist of a copy of a pipeline with all of its parameter values and a run trigger. A run trigger indicates when a recurring run executes. You can define the following run triggers:</p>\n</div>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>Periodic: used for scheduling runs to execute in intervals.</p>\n</li>\n<li>\n<p>Cron: used for scheduling runs as a cron job.</p>\n</li>\n</ul>\n</div>\n<div class=\"paragraph\">\n<p>When executed, you can track the run&#8217;s progress from the run&#8217;s <strong>Details</strong> page on the {productname-short} user interface. From here, you can view the run&#8217;s graph, and output artifacts.</p>\n</div>\n<div class=\"paragraph\">\n<p>A pipeline run can be classified as the following:</p>\n</div>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>Scheduled run: A pipeline run scheduled to execute at least once</p>\n</li>\n<li>\n<p>Triggered run: A previously executed pipeline run.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"scheduling-a-pipeline-run_ds-pipelines\">Scheduling a pipeline run</h3>\n<div class=\"paragraph _abstract\">\n<p>You can instantiate a single execution of a pipeline by scheduling a pipeline run. In {productname-short}, you can schedule runs to occur at specific times or execute them immediately after creation.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have installed the OpenShift Pipelines operator.</p>\n</li>\n<li>\n<p>You have logged in to {productname-long}.</p>\n</li>\n<li>\n<p>If you are using specialized {productname-short} groups, you are part of the user group or admin group (for example, <code>odh-users</code> or <code>odh-admins</code>) in OpenShift.</p>\n</li>\n<li>\n<p>You have previously created a data science project that is available and contains a configured pipeline server.</p>\n</li>\n<li>\n<p>You have imported a pipeline to an active pipeline server.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From the {productname-short} dashboard, click <strong>Data Science Pipelines</strong> &#8594; <strong>Pipelines</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Pipelines</strong> page opens.</p>\n</div>\n</li>\n<li>\n<p>Click the action menu (<strong>&#8942;</strong>) beside the relevant pipeline and click <strong>Create run</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Create run</strong> page opens.</p>\n</div>\n</li>\n<li>\n<p>From the <strong>Project</strong> list, select the project that contains the pipeline you want to create a run for.</p>\n</li>\n<li>\n<p>In the <strong>Name</strong> field, enter a name for the run.</p>\n</li>\n<li>\n<p>In the <strong>Description</strong> field, enter a description for the run.</p>\n</li>\n<li>\n<p>From the <strong>Pipeline</strong> list, select the pipeline to create a run for. Alternatively, to upload a new pipeline, click <strong>Upload new pipeline</strong> and fill in the relevant fields in the <strong>Import pipeline</strong> dialog.</p>\n</li>\n<li>\n<p>Configure the run type by performing one of the following sets of actions:</p>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>Select <strong>Run once immediately after creation</strong> to specify the run executes once, and immediately after its creation.</p>\n</li>\n<li>\n<p>Select <strong>Schedule recurring run</strong> to schedule the run to recur.</p>\n<div class=\"olist lowerroman\">\n<ol class=\"lowerroman\" type=\"i\">\n<li>\n<p>Configure the run&#8217;s trigger type.</p>\n<div class=\"olist upperalpha\">\n<ol class=\"upperalpha\" type=\"A\">\n<li>\n<p>Select <strong>Periodic</strong> and select the execution frequency from the list.</p>\n</li>\n<li>\n<p>Select <strong>Cron</strong> to specify the execution schedule in <code>cron</code> format. This creates a cron job to execute the run. Click the <strong>Copy</strong> button (<span class=\"image\"><img src=\"/static/docs/images/osd-copy.png\" alt=\"osd copy\"></span>) to copy the cron job schedule to the clipboard. The field furthest to the left represents seconds. For more information about scheduling tasks using the supported <code>cron</code> format, see <a href=\"https://pkg.go.dev/github.com/robfig/cron#hdr-CRON_Expression_Format\">Cron Expression Format</a>.</p>\n</li>\n</ol>\n</div>\n</li>\n<li>\n<p>Configure the run&#8217;s duration.</p>\n<div class=\"olist upperalpha\">\n<ol class=\"upperalpha\" type=\"A\">\n<li>\n<p>Select the <strong>Start date</strong> check box to specify a start date for the run. Select the run&#8217;s start date using the <strong>Calendar</strong> and the start time from the list of times.</p>\n</li>\n<li>\n<p>Select the <strong>End date</strong> check box to specify an end date for the run. Select the run&#8217;s end date using the <strong>Calendar</strong> and the end time from the list of times.</p>\n</li>\n</ol>\n</div>\n</li>\n</ol>\n</div>\n</li>\n</ul>\n</div>\n</li>\n<li>\n<p>Configure the input parameters for the run by selecting the parameters from the list.</p>\n</li>\n<li>\n<p>Click <strong>Create</strong>.</p>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>The pipeline run that you created is shown in the <strong>Scheduled</strong> tab on the <strong>Runs</strong> page.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"cloning-a-scheduled-pipeline-run_ds-pipelines\">Cloning a scheduled pipeline run</h3>\n<div class=\"paragraph _abstract\">\n<p>To make it easier to schedule runs to execute as part of your pipeline configuration, you can duplicate existing scheduled runs by cloning them.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have installed the OpenShift Pipelines operator.</p>\n</li>\n<li>\n<p>You have logged in to {productname-long}.</p>\n</li>\n<li>\n<p>If you are using specialized {productname-short} groups, you are part of the user group or admin group (for example, <code>odh-users</code> or <code>odh-admins</code>) in OpenShift.</p>\n</li>\n<li>\n<p>You have previously created a data science project that is available and contains a configured pipeline server.</p>\n</li>\n<li>\n<p>You have imported a pipeline to an active pipeline server.</p>\n</li>\n<li>\n<p>You have previously scheduled a run that is available to clone.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From the {productname-short} dashboard, click <strong>Data Science Pipelines</strong> &#8594; <strong>Runs</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Runs</strong> page opens.</p>\n</div>\n</li>\n<li>\n<p>Click the action menu (<strong>&#8942;</strong>) beside the relevant run and click <strong>Clone</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Clone</strong> page opens.</p>\n</div>\n</li>\n<li>\n<p>From the <strong>Project</strong> list, select the project that contains the pipeline whose run that you want to clone.</p>\n</li>\n<li>\n<p>In the <strong>Name</strong> field, enter a name for the run that you want to clone.</p>\n</li>\n<li>\n<p>In the <strong>Description</strong> field, enter a description for the run that you want to clone.</p>\n</li>\n<li>\n<p>From the <strong>Pipeline</strong> list, select the pipeline containing the run that you want to clone.</p>\n</li>\n<li>\n<p>To configure the run type for the run that you are cloning, in the <strong>Run type</strong> section, perform one of the following sets of actions:</p>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>Select <strong>Run once immediately after create</strong> to specify the run that you are cloning executes once, and immediately after its creation. If you selected this option, skip to step 10.</p>\n</li>\n<li>\n<p>Select <strong>Schedule recurring run</strong> to schedule the run that you are cloning to recur.</p>\n</li>\n</ul>\n</div>\n</li>\n<li>\n<p>If you selected <strong>Schedule recurring run</strong> in the previous step, configure the trigger type for the run, perform one of the following actions:</p>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>Select <strong>Periodic</strong> and select the execution frequency from the <strong>Run every</strong> list.</p>\n</li>\n<li>\n<p>Select <strong>Cron</strong> to specify the execution schedule in <code>cron</code> format. This creates a cron job to execute the run. Click the <strong>Copy</strong> button (<span class=\"image\"><img src=\"/static/docs/images/osd-copy.png\" alt=\"osd copy\"></span>) to copy the cron job schedule to the clipboard. The field furthest to the left represents seconds. For more information about scheduling tasks using the supported <code>cron</code> format, see <a href=\"https://pkg.go.dev/github.com/robfig/cron#hdr-CRON_Expression_Format\">Cron Expression Format</a>.</p>\n</li>\n</ul>\n</div>\n</li>\n<li>\n<p>If you selected <strong>Schedule recurring run</strong> in step 7, configure the duration for the run that you are cloning.</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>Select the <strong>Start date</strong> check box to specify a start date for the run. Select the start date using the calendar tool and the start time from the list of times.</p>\n</li>\n<li>\n<p>Select the <strong>End date</strong> check box to specify an end date for the run. Select the end date using the calendar tool and the end time from the list of times.</p>\n</li>\n</ol>\n</div>\n</li>\n<li>\n<p>In the <strong>Parameters</strong> section, configure the input parameters for the run that you are cloning by selecting the appropriate parameters from the list.</p>\n</li>\n<li>\n<p>Click <strong>Create</strong>.</p>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>The pipeline run that you cloned is shown in the <strong>Scheduled</strong> tab on the <strong>Runs</strong> page.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"stopping-a-triggered-pipeline-run_ds-pipelines\">Stopping a triggered pipeline run</h3>\n<div class=\"paragraph _abstract\">\n<p>If you no longer require a triggered pipeline run to continue executing, you can stop the run before its defined end date.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have installed the OpenShift Pipelines operator.</p>\n</li>\n<li>\n<p>You have logged in to {productname-long}.</p>\n</li>\n<li>\n<p>If you are using specialized {productname-short} groups, you are part of the user group or admin group (for example, <code>odh-users</code> or <code>odh-admins</code>) in OpenShift.</p>\n</li>\n<li>\n<p>There is a previously created data science project available that contains a pipeline server.</p>\n</li>\n<li>\n<p>You have imported a pipeline to an active and available pipeline server.</p>\n</li>\n<li>\n<p>You have previously triggered a pipeline run.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From the {productname-short} dashboard, click <strong>Data Science Pipelines</strong> &#8594; <strong>Runs</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Runs</strong> page opens.</p>\n</div>\n</li>\n<li>\n<p>From the <strong>Project</strong> list, select the project whose pipeline runs you want to stop.</p>\n</li>\n<li>\n<p>Click the <strong>Triggered</strong> tab.</p>\n</li>\n<li>\n<p>In the <strong>Name</strong> column in the table, click the name of the run that you want to stop.</p>\n<div class=\"paragraph\">\n<p>The <strong>Run details</strong> page opens.</p>\n</div>\n</li>\n<li>\n<p>From the <strong>Actions</strong> list, select <strong>Stop run</strong></p>\n<div class=\"paragraph\">\n<p>There might be a short delay while the run stops.</p>\n</div>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>A list of previously triggered runs are displayed in the <strong>Triggered</strong> tab on the <strong>Runs</strong> page.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"deleting-a-scheduled-pipeline-run_ds-pipelines\">Deleting a scheduled pipeline run</h3>\n<div class=\"paragraph _abstract\">\n<p>To discard pipeline runs that you previously scheduled, but no longer require, you can delete them so that they do not appear on the <strong>Runs</strong> page.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have installed the OpenShift Pipelines operator.</p>\n</li>\n<li>\n<p>You have logged in to {productname-long}.</p>\n</li>\n<li>\n<p>If you are using specialized {productname-short} groups, you are part of the user group or admin group (for example, <code>odh-users</code> or <code>odh-admins</code>) in OpenShift.</p>\n</li>\n<li>\n<p>You have previously created a data science project that is available and contains a configured pipeline server.</p>\n</li>\n<li>\n<p>You have imported a pipeline to an active pipeline server.</p>\n</li>\n<li>\n<p>You have previously scheduled a run that is available to delete.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From the {productname-short} dashboard, click <strong>Data Science Pipelines</strong> &#8594; <strong>Runs</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Runs</strong> page opens.</p>\n</div>\n</li>\n<li>\n<p>From the <strong>Project</strong> list, select the project that contains the pipeline whose scheduled run you want to delete.</p>\n<div class=\"paragraph\">\n<p>The page refreshes to show the pipeline&#8217;s scheduled runs on the <strong>Scheduled</strong> tab.</p>\n</div>\n</li>\n<li>\n<p>Click the action menu (<strong>&#8942;</strong>) beside the scheduled run that you want to delete and click <strong>Delete</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Delete scheduled run</strong> dialog opens.</p>\n</div>\n</li>\n<li>\n<p>Enter the run&#8217;s name in the text field to confirm that you intend to delete it.</p>\n</li>\n<li>\n<p>Click <strong>Delete scheduled run</strong>.</p>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>The run that you deleted is no longer displayed on the <strong>Scheduled</strong> tab.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"deleting-a-triggered-pipeline-run_ds-pipelines\">Deleting a triggered pipeline run</h3>\n<div class=\"paragraph _abstract\">\n<p>To discard pipeline runs that you previously executed, but no longer require a record of, you can delete them so that they do not appear on the <strong>Triggered</strong> tab on the <strong>Runs</strong> page.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have installed the OpenShift Pipelines operator.</p>\n</li>\n<li>\n<p>You have logged in to {productname-long}.</p>\n</li>\n<li>\n<p>If you are using specialized {productname-short} groups, you are part of the user group or admin group (for example, <code>odh-users</code> or <code>odh-admins</code>) in OpenShift.</p>\n</li>\n<li>\n<p>You have previously created a data science project that is available and contains a configured pipeline server.</p>\n</li>\n<li>\n<p>You have imported a pipeline to an active pipeline server.</p>\n</li>\n<li>\n<p>You have previously executed a run that is available to delete.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From the {productname-short} dashboard, click <strong>Data Science Pipelines</strong> &#8594; <strong>Runs</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Runs</strong> page opens.</p>\n</div>\n</li>\n<li>\n<p>From the <strong>Project</strong> list, select the project that contains the pipeline whose triggered run you want to delete.</p>\n<div class=\"paragraph\">\n<p>The page refreshes to show the pipeline&#8217;s triggered runs on the <strong>Triggered</strong> tab.</p>\n</div>\n</li>\n<li>\n<p>Click the action menu (<strong>&#8942;</strong>) beside the triggered run that you want to delete and click <strong>Delete</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Delete triggered run</strong> dialog opens.</p>\n</div>\n</li>\n<li>\n<p>Enter the run&#8217;s name in the text field to confirm that you intend to delete it.</p>\n</li>\n<li>\n<p>Click <strong>Delete triggered run</strong>.</p>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>The run that you deleted is no longer displayed on the <strong>Triggered</strong> tab.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"viewing-scheduled-pipeline-runs_ds-pipelines\">Viewing scheduled pipeline runs</h3>\n<div class=\"paragraph _abstract\">\n<p>You can view a list of pipeline runs that are scheduled for execution in {productname-short}. From this list, you can view details relating to your pipeline&#8217;s runs, such as the pipeline that the run belongs to. You can also view the run&#8217;s status, execution frequency, and schedule.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to {productname-long}.</p>\n</li>\n<li>\n<p>You have installed the Data Science Pipelines operator.</p>\n</li>\n<li>\n<p>If you are using specialized {productname-short} groups, you are part of the user group or admin group (for example, <code>odh-users</code> or <code>odh-admins</code>) in OpenShift.</p>\n</li>\n<li>\n<p>You have previously created a data science project that is available and contains a pipeline server.</p>\n</li>\n<li>\n<p>You have imported a pipeline to an active and available pipeline server.</p>\n</li>\n<li>\n<p>You have created and scheduled a pipeline run.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From the {productname-short} dashboard, click <strong>Data Science Pipelines</strong> &#8594; <strong>Runs</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Runs</strong> page opens.</p>\n</div>\n</li>\n<li>\n<p>From the <strong>Project</strong> list, select the project whose scheduled pipeline runs you want to view.</p>\n</li>\n<li>\n<p>Click the <strong>Scheduled</strong> tab.</p>\n</li>\n<li>\n<p>Study the table showing a list of scheduled runs.</p>\n<div class=\"paragraph\">\n<p>After a run has been scheduled, the run&#8217;s status is displayed in the <strong>Status</strong> column in the table, indicating whether the run is ready for execution or unavailable for execution. To enable or disable a previously imported notebook image, on the row containing the relevant notebook image, click the toggle in the <strong>Enabled</strong> column.</p>\n</div>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>A list of scheduled runs are displayed in the <strong>Scheduled</strong> tab on the <strong>Runs</strong> page.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"viewing-triggered-pipeline-runs_ds-pipelines\">Viewing triggered pipeline runs</h3>\n<div class=\"paragraph _abstract\">\n<p>You can view a list of pipeline runs that were previously executed in {productname-short}. From this list, you can view details relating to your pipeline&#8217;s runs, such as the pipeline that the run belongs to, along with the run&#8217;s status, duration, and execution start time.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to {productname-long}.</p>\n</li>\n<li>\n<p>You have installed the Data Science Pipelines operator.</p>\n</li>\n<li>\n<p>If you are using specialized {productname-short} groups, you are part of the user group or admin group (for example, <code>odh-users</code> or <code>odh-admins</code>) in OpenShift.</p>\n</li>\n<li>\n<p>You have previously created a data science project that is available and contains a pipeline server.</p>\n</li>\n<li>\n<p>You have imported a pipeline to an active and available pipeline server.</p>\n</li>\n<li>\n<p>You have previously triggered a pipeline run.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From the {productname-short} dashboard, click <strong>Data Science Pipelines</strong> &#8594; <strong>Runs</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Runs</strong> page opens.</p>\n</div>\n</li>\n<li>\n<p>From the <strong>Project</strong> list, select the project whose previously executed pipeline runs you want to view.</p>\n<div class=\"paragraph\">\n<p>The <strong>Run details</strong> page opens.</p>\n</div>\n</li>\n<li>\n<p>Click the <strong>Triggered</strong> tab.</p>\n<div class=\"paragraph\">\n<p>A table opens that shows list of triggered runs. After a run has completed its execution, the run&#8217;s status is displayed in the <strong>Status</strong> column in the table, indicating whether the run has succeeded or failed.</p>\n</div>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>A list of previously triggered runs are displayed in the <strong>Triggered</strong> tab on the <strong>Runs</strong> page.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"viewing-the-details-of-a-pipeline-run_ds-pipelines\">Viewing the details of a pipeline run</h3>\n<div class=\"paragraph _abstract\">\n<p>To gain a clearer understanding of your pipeline runs, you can view the details of a previously triggered pipeline run, such as its graph, execution details, and run output.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have installed the OpenShift Pipelines operator.</p>\n</li>\n<li>\n<p>You have logged in to {productname-long}.</p>\n</li>\n<li>\n<p>If you are using specialized {productname-short} groups, you are part of the user group or admin group (for example, <code>odh-users</code> or <code>odh-admins</code>) in OpenShift.</p>\n</li>\n<li>\n<p>You have previously created a data science project that is available and contains a pipeline server.</p>\n</li>\n<li>\n<p>You have imported a pipeline to an active and available pipeline server.</p>\n</li>\n<li>\n<p>You have previously triggered a pipeline run.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From the {productname-short} dashboard, click <strong>Data Science Pipelines</strong> &#8594; <strong>Pipelines</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Pipelines</strong> page opens.</p>\n</div>\n</li>\n<li>\n<p>From the <strong>Project</strong> list, select the project whose pipeline runs you want to view.</p>\n</li>\n<li>\n<p>For a pipeline that you want to see run details for, click <strong>Expand</strong> (<span class=\"image\"><img src=\"/static/docs/images/rhods-expand-icon.png\" alt=\"rhods expand icon\"></span>).</p>\n</li>\n<li>\n<p>From the <strong>Runs</strong> section, click the name of the run that you want to view the details of.</p>\n<div class=\"paragraph\">\n<p>The <strong>Run details</strong> page opens.</p>\n</div>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>On the <strong>Run details</strong> page, you can view the run&#8217;s graph, execution details, input parameters, and run output.</p>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>\n<div class=\"sect1\">\n<h2 id=\"_working_with_pipelines_in_jupyterlab\">Working with pipelines in JupyterLab</h2>\n<div class=\"sectionbody\">\n<div class=\"sect2\">\n<h3 id=\"overview-of-pipelines-in-jupyterlab_ds-pipelines\">Overview of pipelines in JupyterLab</h3>\n<div class=\"paragraph _abstract\">\n<p>You can use Elyra to create visual end-to-end pipeline workflows in JupyterLab. Elyra is an extension for JupyterLab that provides you with a Pipeline Editor to create pipeline workflows that can be executed in {productname-short}.</p>\n</div>\n<div class=\"paragraph\">\n<p>Before you can work with pipelines in JupyterLab, you must install the Data Science Pipelines operator as described in <a href=\"https://github.com/opendatahub-io/data-science-pipelines-operator\">Data Science Pipelines Operator</a>.</p>\n</div>\n<div class=\"paragraph\">\n<p>You can access the Elyra extension within JupyterLab when you create the most recent version of one of the following notebook images:</p>\n</div>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>Standard Data Science</p>\n</li>\n<li>\n<p>PyTorch</p>\n</li>\n<li>\n<p>TensorFlow</p>\n</li>\n<li>\n<p>TrustyAI</p>\n</li>\n</ul>\n</div>\n<div class=\"paragraph\">\n<p>As you can use the Pipeline Editor to visually design your pipelines, minimal coding is required to create and run pipelines. For more information about Elyra, see <a href=\"https://elyra.readthedocs.io/en/stable/getting_started/overview.html\">Elyra Documentation</a>. For more information on the Pipeline Editor, see <a href=\"https://elyra.readthedocs.io/en/stable/user_guide/jupyterlab-interface.html#visual-pipeline-editor\">Visual Pipeline Editor</a>. After you have created your pipeline, you can run it locally in JupyterLab, or remotely using data science pipelines in {productname-short}.</p>\n</div>\n<div class=\"paragraph\">\n<p>The pipeline creation process consists of the following tasks:</p>\n</div>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>Create a data science project that contains a workbench.</p>\n</li>\n<li>\n<p>Create a pipeline server.</p>\n</li>\n<li>\n<p>Create a new pipeline in the Pipeline Editor in JupyterLab.</p>\n</li>\n<li>\n<p>Develop your pipeline by adding Python notebooks or Python scripts and defining their runtime properties.</p>\n</li>\n<li>\n<p>Define execution dependencies.</p>\n</li>\n<li>\n<p>Run or export your pipeline.</p>\n</li>\n</ul>\n</div>\n<div class=\"paragraph\">\n<p>Before you can run a pipeline in JupyterLab, your pipeline instance must contain a runtime configuration. A runtime configuration defines connectivity information for your pipeline instance and S3-compatible cloud storage.</p>\n</div>\n<div class=\"paragraph\">\n<p>If you create a workbench as part of a data science project, a default runtime configuration is created automatically. However, if you create a notebook from the Jupyter tile in the {productname-short} dashboard, you must create a runtime configuration before you can run your pipeline in JupyterLab. For more information about runtime configurations, see <a href=\"https://elyra.readthedocs.io/en/stable/user_guide/runtime-conf.html\">Runtime Configuration</a>. As a prerequisite, before you create a workbench, ensure that you have created and configured a pipeline server within the same data science project as your workbench.</p>\n</div>\n<div class=\"paragraph\">\n<p>You can use S3-compatible cloud storage to make data available to your notebooks and scripts while they are executed. Your cloud storage must be accessible from the machine in your deployment that runs JupyterLab and from the cluster that hosts Data Science Pipelines. Before you create and run pipelines in JupyterLab, ensure that you have your s3-compatible storage credentials readily available.</p>\n</div>\n<div class=\"ulist _additional-resources\">\n<div class=\"title\">Additional resources</div>\n<ul>\n<li>\n<p><a href=\"https://elyra.readthedocs.io/en/stable/getting_started/overview.html\">Elyra Documentation</a></p>\n</li>\n<li>\n<p><a href=\"https://elyra.readthedocs.io/en/stable/user_guide/jupyterlab-interface.html#visual-pipeline-editor\">Visual Pipeline Editor</a></p>\n</li>\n<li>\n<p><a href=\"https://elyra.readthedocs.io/en/stable/user_guide/runtime-conf.html\">Runtime Configuration</a>.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"accessing-the-pipeline-editor_ds-pipelines\">Accessing the pipeline editor</h3>\n<div class=\"paragraph _abstract\">\n<p>You can use Elyra to create visual end-to-end pipeline workflows in JupyterLab. Elyra is an extension for JupyterLab that provides you with a Pipeline Editor to create pipeline workflows that can be executed in {productname-short}.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have installed the OpenShift Pipelines operator.</p>\n</li>\n<li>\n<p>You have logged in to {productname-long}.</p>\n</li>\n<li>\n<p>If you are using specialized {productname-short} groups, you are part of the user group or admin group (for example, <code>odh-users</code> or <code>odh-admins</code>) in OpenShift.</p>\n</li>\n<li>\n<p>You have created a data science project that contains a workbench.</p>\n</li>\n<li>\n<p>You have created and configured a pipeline server within the data science project that contains your workbench.</p>\n</li>\n<li>\n<p>You have created and launched a Jupyter server from a notebook image that contains the Elyra extension (Standard data science, TensorFlow, TrustyAI, or  PyTorch).</p>\n</li>\n<li>\n<p>You have access to S3-compatible storage.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>After you open JupyterLab, confirm that the JupyterLab launcher is automatically displayed.</p>\n</li>\n<li>\n<p>In the <strong>Elyra</strong> section of the JupyterLab launcher, click the <strong>Pipeline Editor</strong> tile.</p>\n<div class=\"paragraph\">\n<p>The Pipeline Editor opens.</p>\n</div>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>You can view the Pipeline Editor in JupyterLab.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"creating-a-runtime-configuration_ds-pipelines\">Creating a runtime configuration</h3>\n<div class=\"paragraph _abstract\">\n<p>If you create a workbench as part of a data science project, a default runtime configuration is created automatically. However, if you create a notebook from the Jupyter tile in the {productname-short} dashboard, you must create a runtime configuration before you can run your pipeline in JupyterLab. This enables you to specify connectivity information for your pipeline instance and S3-compatible cloud storage.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have installed the OpenShift Pipelines operator.</p>\n</li>\n<li>\n<p>You have logged in to {productname-long}.</p>\n</li>\n<li>\n<p>If you are using specialized {productname-short} groups, you are part of the user group or admin group (for example, <code>odh-users</code> or <code>odh-admins</code>) in OpenShift.</p>\n</li>\n<li>\n<p>You have access to S3-compatible cloud storage.</p>\n</li>\n<li>\n<p>You have created a data science project that contains a workbench.</p>\n</li>\n<li>\n<p>You have created and configured a pipeline server within the data science project that contains your workbench.</p>\n</li>\n<li>\n<p>You have created and launched a Jupyter server from a notebook image that contains the Elyra extension (Standard data science, TensorFlow, TrustyAI, or  PyTorch).</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>In the left sidebar of JupyterLab, click <strong>Runtimes</strong> (<span class=\"image\"><img src=\"/static/docs/images/jupyter-runtimes-sidebar.png\" alt=\"The Runtimes icon\"></span>).</p>\n</li>\n<li>\n<p>Click the <strong>Create new runtime configuration</strong> button (<span class=\"image\"><img src=\"/static/docs/images/jupyter-create-runtime.png\" alt=\"Create new runtime configuration\"></span>).</p>\n<div class=\"paragraph\">\n<p>The <strong>Add new Data Science Pipelines runtime configuration</strong> page opens.</p>\n</div>\n</li>\n<li>\n<p>Fill in the relevant fields to define your runtime configuration.</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>In the <strong>Display Name</strong> field, enter a name for your runtime configuration.</p>\n</li>\n<li>\n<p>Optional: In the <strong>Description</strong> field, enter a description to define your runtime configuration.</p>\n</li>\n<li>\n<p>Optional: In the <strong>Tags</strong> field, click <strong>Add Tag</strong> to define a category for your pipeline instance. Enter a name for the tag and press Enter.</p>\n</li>\n<li>\n<p>Define the credentials of your data science pipeline:</p>\n<div class=\"olist lowerroman\">\n<ol class=\"lowerroman\" type=\"i\">\n<li>\n<p>In the <strong>Data Science Pipelines API Endpoint</strong> field, enter the API endpoint of your data science pipeline. Do not specify the pipelines namespace in this field.</p>\n</li>\n<li>\n<p>In the <strong>Public Data Science Pipelines API Endpoint</strong> field, enter the public API endpoint of your data science pipeline.</p>\n<div class=\"admonitionblock important\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Important</div>\n</td>\n<td class=\"content\">\n<div class=\"paragraph\">\n<p>You can obtain the Data Science Pipelines API endpoint from the <strong>Data Science Pipelines</strong> &#8594; <strong>Runs</strong> page in the dashboard. Copy the relevant end point and enter it in the <strong>Public Data Science Pipelines API Endpoint</strong> field.</p>\n</div>\n</td>\n</tr>\n</table>\n</div>\n</li>\n<li>\n<p>Optional: In the <strong>Data Science Pipelines User Namespace</strong> field, enter the relevant user namespace to run pipelines.</p>\n</li>\n<li>\n<p>From the <strong>Data Science Pipelines engine</strong> list, select <code>Tekton</code>.</p>\n</li>\n<li>\n<p>From the <strong>Authentication Type</strong> list, select the authentication type required to authenticate your pipeline.</p>\n<div class=\"admonitionblock important\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Important</div>\n</td>\n<td class=\"content\">\n<div class=\"paragraph\">\n<p>If you created a notebook directly from the Jupyter tile on the dashboard, select <code>EXISTING_BEARER_TOKEN</code> from the <strong>Authentication Type</strong> list.</p>\n</div>\n</td>\n</tr>\n</table>\n</div>\n</li>\n<li>\n<p>In the <strong>Data Science Pipelines API Endpoint Username</strong> field, enter the user name required for the authentication type.</p>\n</li>\n<li>\n<p>In the <strong>Data Science Pipelines API Endpoint Password Or Token</strong>, enter the password or token required for the authentication type.</p>\n<div class=\"admonitionblock important\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Important</div>\n</td>\n<td class=\"content\">\n<div class=\"paragraph\">\n<p>To obtain the Data Science Pipelines API endpoint token, in the upper-right corner of the OpenShift web console, click your user name and select <strong>Copy login command</strong>. After you have logged in, click <strong>Display token</strong> and copy the value of <code>--token=</code> from the <strong>Log in with this token</strong> command.</p>\n</div>\n</td>\n</tr>\n</table>\n</div>\n</li>\n</ol>\n</div>\n</li>\n<li>\n<p>Define the connectivity information of your S3-compatible storage:</p>\n<div class=\"olist lowerroman\">\n<ol class=\"lowerroman\" type=\"i\">\n<li>\n<p>In the <strong>Cloud Object Storage Endpoint</strong> field, enter the endpoint of your S3-compatible storage. For more information about Amazon s3 endpoints, see <a href=\"https://docs.aws.amazon.com/general/latest/gr/s3.html\">Amazon Simple Storage Service endpoints and quotas</a>.</p>\n</li>\n<li>\n<p>Optional: In the <strong>Public Cloud Object Storage Endpoint</strong> field, enter the URL of your S3-compatible storage.</p>\n</li>\n<li>\n<p>In the <strong>Cloud Object Storage Bucket Name</strong> field, enter the name of the bucket where your pipeline artifacts are stored. If the bucket name does not exist, it is created automatically.</p>\n</li>\n<li>\n<p>From the <strong>Cloud Object Storage Authentication Type</strong> list, select the authentication type required to access to your S3-compatible cloud storage. If you use AWS S3 buckets, select <code>KUBERNETES_SECRET</code> from the list.</p>\n</li>\n<li>\n<p>In the <strong>Cloud Object Storage Credentials Secret</strong> field, enter the secret that contains the storage user name and password. This secret is defined in the relevant user namespace, if applicable. In addition, it must be stored on the cluster that hosts your pipeline runtime.</p>\n</li>\n<li>\n<p>In the <strong>Cloud Object Storage Username</strong> field, enter the user name to connect to your S3-compatible cloud storage, if applicable. If you use AWS S3 buckets, enter your AWS Secret Access Key ID.</p>\n</li>\n<li>\n<p>In the <strong>Cloud Object Storage Password</strong> field, enter the password to connect to your S3-compatible cloud storage, if applicable. If you use AWS S3 buckets, enter your AWS Secret Access Key.</p>\n</li>\n</ol>\n</div>\n</li>\n<li>\n<p>Click <strong>Save &amp; Close</strong>.</p>\n</li>\n</ol>\n</div>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>The runtime configuration that you created is shown in the <strong>Runtimes</strong> tab (<span class=\"image\"><img src=\"/static/docs/images/jupyter-runtimes-sidebar.png\" alt=\"The Runtimes icon\"></span>) in the left sidebar of JupyterLab.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"updating-a-runtime-configuration_ds-pipelines\">Updating a runtime configuration</h3>\n<div class=\"paragraph _abstract\">\n<p>To ensure that your runtime configuration is accurate and updated, you can change the settings of an existing runtime configuration.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have installed the OpenShift Pipelines operator.</p>\n</li>\n<li>\n<p>You have logged in to {productname-long}.</p>\n</li>\n<li>\n<p>If you are using specialized {productname-short} groups, you are part of the user group or admin group (for example, <code>odh-users</code> or <code>odh-admins</code>) in OpenShift.</p>\n</li>\n<li>\n<p>You have access to S3-compatible storage.</p>\n</li>\n<li>\n<p>You have created a data science project that contains a workbench.</p>\n</li>\n<li>\n<p>You have created and configured a pipeline server within the data science project that contains your workbench.</p>\n</li>\n<li>\n<p>A previously created runtime configuration is available in the JupyterLab interface.</p>\n</li>\n<li>\n<p>You have created and launched a Jupyter server from a notebook image that contains the Elyra extension (Standard data science, TensorFlow, TrustyAI, or  PyTorch).</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>In the left sidebar of JupyterLab, click <strong>Runtimes</strong> (<span class=\"image\"><img src=\"/static/docs/images/jupyter-runtimes-sidebar.png\" alt=\"The Runtimes icon\"></span>).</p>\n</li>\n<li>\n<p>Hover the cursor over the runtime configuration that you want to update and click the <strong>Edit</strong> button (<span class=\"image\"><img src=\"/static/docs/images/rhods-edit-icon.png\" alt=\"Edit runtime configuration\"></span>).</p>\n<div class=\"paragraph\">\n<p>The <strong>Data Science Pipelines runtime configuration</strong> page opens.</p>\n</div>\n</li>\n<li>\n<p>Fill in the relevant fields to update your runtime configuration.</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>In the <strong>Display Name</strong> field, update name for your runtime configuration, if applicable.</p>\n</li>\n<li>\n<p>Optional: In the <strong>Description</strong> field, update the description of your runtime configuration, if applicable.</p>\n</li>\n<li>\n<p>Optional: In the <strong>Tags</strong> field, click <strong>Add Tag</strong> to define a category for your pipeline instance. Enter a name for the tag and press Enter.</p>\n</li>\n<li>\n<p>Define the credentials of your data science pipeline:</p>\n<div class=\"olist lowerroman\">\n<ol class=\"lowerroman\" type=\"i\">\n<li>\n<p>In the <strong>Data Science Pipelines API Endpoint</strong> field, update the API endpoint of your data science pipeline, if applicable. Do not specify the pipelines namespace in this field.</p>\n</li>\n<li>\n<p>In the <strong>Public Data Science Pipelines API Endpoint</strong> field, update the API endpoint of your data science pipeline, if applicable.</p>\n</li>\n<li>\n<p>Optional: In the <strong>Data Science Pipelines User Namespace</strong> field, update the relevant user namespace to run pipelines, if applicable.</p>\n</li>\n<li>\n<p>From the <strong>Data Science Pipelines engine</strong> list, select <code>Tekton</code>.</p>\n</li>\n<li>\n<p>From the <strong>Authentication Type</strong> list, select a new authentication type required to authenticate your pipeline, if applicable.</p>\n<div class=\"admonitionblock important\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Important</div>\n</td>\n<td class=\"content\">\n<div class=\"paragraph\">\n<p>If you created a notebook directly from the Jupyter tile on the dashboard, select <code>EXISTING_BEARER_TOKEN</code> from the <strong>Authentication Type</strong> list.</p>\n</div>\n</td>\n</tr>\n</table>\n</div>\n</li>\n<li>\n<p>In the <strong>Data Science Pipelines API Endpoint Username</strong> field, update the user name required for the authentication type, if applicable.</p>\n</li>\n<li>\n<p>In the <strong>Data Science Pipelines API Endpoint Password Or Token</strong>, update the password or token required for the authentication type, if applicable.</p>\n<div class=\"admonitionblock important\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Important</div>\n</td>\n<td class=\"content\">\n<div class=\"paragraph\">\n<p>To obtain the Data Science Pipelines API endpoint token, in the upper-right corner of the OpenShift web console, click your user name and select <strong>Copy login command</strong>. After you have logged in, click <strong>Display token</strong> and copy the value of <code>--token=</code> from the <strong>Log in with this token</strong> command.</p>\n</div>\n</td>\n</tr>\n</table>\n</div>\n</li>\n</ol>\n</div>\n</li>\n<li>\n<p>Define the connectivity information of your S3-compatible storage:</p>\n<div class=\"olist lowerroman\">\n<ol class=\"lowerroman\" type=\"i\">\n<li>\n<p>In the <strong>Cloud Object Storage Endpoint</strong> field, update the endpoint of your S3-compatible storage, if applicable. For more information about Amazon s3 endpoints, see <a href=\"https://docs.aws.amazon.com/general/latest/gr/s3.html\">Amazon Simple Storage Service endpoints and quotas</a>.</p>\n</li>\n<li>\n<p>Optional: In the <strong>Public Cloud Object Storage Endpoint</strong> field, update the URL of your S3-compatible storage, if applicable.</p>\n</li>\n<li>\n<p>In the <strong>Cloud Object Storage Bucket Name</strong> field, update the name of the bucket where your pipeline artifacts are stored, if applicable. If the bucket name does not exist, it is created automatically.</p>\n</li>\n<li>\n<p>From the <strong>Cloud Object Storage Authentication Type</strong> list, update the authentication type required to access to your S3-compatible cloud storage, if applicable. If you use AWS S3 buckets, you must select <code>USER_CREDENTIALS</code> from the list.</p>\n</li>\n<li>\n<p>Optional: In the <strong>Cloud Object Storage Credentials Secret</strong> field, update the secret that contains the storage user name and password, if applicable. This secret is defined in the relevant user namespace. You must save the secret on the cluster that hosts your pipeline runtime.</p>\n</li>\n<li>\n<p>Optional: In the <strong>Cloud Object Storage Username</strong> field, update the user name to connect to your S3-compatible cloud storage, if applicable. If you use AWS S3 buckets, update your AWS Secret Access Key ID.</p>\n</li>\n<li>\n<p>Optional: In the <strong>Cloud Object Storage Password</strong> field, update the password to connect to your S3-compatible cloud storage, if applicable. If you use AWS S3 buckets, update your AWS Secret Access Key.</p>\n</li>\n</ol>\n</div>\n</li>\n<li>\n<p>Click <strong>Save &amp; Close</strong>.</p>\n</li>\n</ol>\n</div>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>The runtime configuration that you updated is shown in the <strong>Runtimes</strong> tab (<span class=\"image\"><img src=\"/static/docs/images/jupyter-runtimes-sidebar.png\" alt=\"The Runtimes icon\"></span>) in the left sidebar of JupyterLab.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"deleting-a-runtime-configuration_ds-pipelines\">Deleting a runtime configuration</h3>\n<div class=\"paragraph _abstract\">\n<p>After you have finished using your runtime configuration, you can delete it from the JupyterLab interface. After deleting a runtime configuration, you cannot run pipelines in JupyterLab until you create another runtime configuration.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have installed the OpenShift Pipelines operator.</p>\n</li>\n<li>\n<p>You have logged in to {productname-long}.</p>\n</li>\n<li>\n<p>If you are using specialized {productname-short} groups, you are part of the user group or admin group (for example, <code>odh-users</code> or <code>odh-admins</code>) in OpenShift.</p>\n</li>\n<li>\n<p>You have created a data science project that contains a workbench.</p>\n</li>\n<li>\n<p>You have created and configured a pipeline server within the data science project that contains your workbench.</p>\n</li>\n<li>\n<p>A previously created runtime configuration is visible in the JupyterLab interface.</p>\n</li>\n<li>\n<p>You have created and launched a Jupyter server from a notebook image that contains the Elyra extension (Standard data science, TensorFlow, TrustyAI, or  PyTorch).</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>In the left sidebar of JupyterLab, click <strong>Runtimes</strong> (<span class=\"image\"><img src=\"/static/docs/images/jupyter-runtimes-sidebar.png\" alt=\"The Runtimes icon\"></span>).</p>\n</li>\n<li>\n<p>Hover the cursor over the runtime configuration that you want to delete and click the <strong>Delete Item</strong> button (<span class=\"image\"><img src=\"/static/docs/images/jupyterlab-trash-button.png\" alt=\"Delete item\"></span>).</p>\n<div class=\"paragraph\">\n<p>A dialog box appears prompting you to confirm the deletion of your runtime configuration.</p>\n</div>\n</li>\n<li>\n<p>Click <strong>OK</strong>.</p>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>The runtime configuration that you deleted is no longer shown in the <strong>Runtimes</strong> tab (<span class=\"image\"><img src=\"/static/docs/images/jupyter-runtimes-sidebar.png\" alt=\"The Runtimes icon\"></span>) in the left sidebar of JupyterLab.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"duplicating-a-runtime-configuration_ds-pipelines\">Duplicating a runtime configuration</h3>\n<div class=\"paragraph _abstract\">\n<p>To prevent you from re-creating runtime configurations with similar values in their entirety, you can duplicate an existing runtime configuration in the JupyterLab interface.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have installed the OpenShift Pipelines operator.</p>\n</li>\n<li>\n<p>You have logged in to {productname-long}.</p>\n</li>\n<li>\n<p>If you are using specialized {productname-short} groups, you are part of the user group or admin group (for example, <code>odh-users</code> or <code>odh-admins</code>) in OpenShift.</p>\n</li>\n<li>\n<p>You have created a data science project that contains a workbench.</p>\n</li>\n<li>\n<p>You have created and configured a pipeline server within the data science project that contains your workbench.</p>\n</li>\n<li>\n<p>A previously created runtime configuration is visible in the JupyterLab interface.</p>\n</li>\n<li>\n<p>You have created and launched a Jupyter server from a notebook image that contains the Elyra extension (Standard data science, TensorFlow, TrustyAI, or  PyTorch).</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>In the left sidebar of JupyterLab, click <strong>Runtimes</strong> (<span class=\"image\"><img src=\"/static/docs/images/jupyter-runtimes-sidebar.png\" alt=\"The Runtimes icon\"></span>).</p>\n</li>\n<li>\n<p>Hover the cursor over the runtime configuration that you want to duplicate and click the <strong>Duplicate</strong> button (<span class=\"image\"><img src=\"/static/docs/images/jupyterlab-duplicate.png\" alt=\"Duplicate\"></span>).</p>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>The runtime configuration that you duplicated is shown in the <strong>Runtimes</strong> tab (<span class=\"image\"><img src=\"/static/docs/images/jupyter-runtimes-sidebar.png\" alt=\"The Runtimes icon\"></span>) in the left sidebar of JupyterLab.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"running-a-pipeline-in-jupyterlab_ds-pipelines\">Running a pipeline in JupyterLab</h3>\n<div class=\"paragraph _abstract\">\n<p>You can run pipelines that you have created in JupyterLab from the Pipeline Editor user interface. Before you can run a pipeline, you must create a data science project and a pipeline server. After you create a pipeline server, you must create a workbench within the same project as your pipeline server.\nYour pipeline instance in JupyterLab must contain a runtime configuration. If you create a workbench as part of a data science project, a default runtime configuration is created automatically. However, if you create a notebook from the Jupyter tile in the {productname-short} dashboard, you must create a runtime configuration before you can run your pipeline in JupyterLab. A runtime configuration defines connectivity information for your pipeline instance and S3-compatible cloud storage.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have installed the OpenShift Pipelines operator.</p>\n</li>\n<li>\n<p>You have logged in to {productname-long}.</p>\n</li>\n<li>\n<p>If you are using specialized {productname-short} groups, you are part of the user group or admin group (for example, <code>odh-users</code> or <code>odh-admins</code>) in OpenShift.</p>\n</li>\n<li>\n<p>You have access to S3-compatible storage.</p>\n</li>\n<li>\n<p>You have created a pipeline in JupyterLab.</p>\n</li>\n<li>\n<p>You have opened your pipeline in the Pipeline Editor in JupyterLab.</p>\n</li>\n<li>\n<p>Your pipeline instance contains a runtime configuration.</p>\n</li>\n<li>\n<p>You have created and configured a pipeline server within the data science project that contains your workbench.</p>\n</li>\n<li>\n<p>You have created and launched a Jupyter server from a notebook image that contains the Elyra extension (Standard data science, TensorFlow, TrustyAI, or  PyTorch).</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>In the Pipeline Editor user interface, click <strong>Run Pipeline</strong> (<span class=\"image\"><img src=\"/static/docs/images/jupyterlab-run-pipeline-button.png\" alt=\"The Runtimes icon\"></span>).</p>\n<div class=\"paragraph\">\n<p>The <strong>Run Pipeline</strong> dialog appears. The <strong>Pipeline Name</strong> field is automatically populated with the pipeline file name.</p>\n</div>\n<div class=\"admonitionblock important\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Important</div>\n</td>\n<td class=\"content\">\n<div class=\"paragraph\">\n<p>You must enter a unique pipeline name. The pipeline name that you enter must not match the name of any previously executed pipelines.</p>\n</div>\n</td>\n</tr>\n</table>\n</div>\n</li>\n<li>\n<p>Define the settings for your pipeline run.</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>From the <strong>Runtime Configuration</strong> list, select the relevant runtime configuration to run your pipeline.</p>\n</li>\n<li>\n<p>Optional: Configure your pipeline parameters, if applicable. If your pipeline contains nodes that reference pipeline parameters, you can change the default parameter values. If a parameter is required and has no default value, you must enter a value.</p>\n</li>\n</ol>\n</div>\n</li>\n<li>\n<p>Click <strong>OK</strong>.</p>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>You can view the output artifacts of your pipeline run. The artifacts are stored in your designated object storage bucket.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"exporting-a-pipeline-in-jupyterlab_ds-pipelines\">Exporting a pipeline in JupyterLab</h3>\n<div class=\"paragraph _abstract\">\n<p>You can export pipelines that you have created in JupyterLab. When you export a pipeline, the pipeline is prepared for later execution, but is not uploaded or executed immediately. During the export process, any package dependencies are uploaded to S3-compatible storage. Also, pipeline code is generated for the target runtime.</p>\n</div>\n<div class=\"paragraph\">\n<p>Before you can export a pipeline, you must create a data science project and a pipeline server. After you create a pipeline server, you must create a workbench within the same project as your pipeline server. In addition, your pipeline instance in JupyterLab must contain a runtime configuration. If you create a workbench as part of a data science project, a default runtime configuration is created automatically. However, if you create a notebook from the Jupyter tile in the {productname-short} dashboard, you must create a runtime configuration before you can export your pipeline in JupyterLab. A runtime configuration defines connectivity information for your pipeline instance and S3-compatible cloud storage.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have installed the OpenShift Pipelines operator.</p>\n</li>\n<li>\n<p>You have logged in to {productname-long}.</p>\n</li>\n<li>\n<p>If you are using specialized {productname-short} groups, you are part of the user group or admin group (for example, <code>odh-users</code> or <code>odh-admins</code>) in OpenShift.</p>\n</li>\n<li>\n<p>You have created a data science project that contains a workbench.</p>\n</li>\n<li>\n<p>You have created and configured a pipeline server within the data science project that contains your workbench.</p>\n</li>\n<li>\n<p>You have access to S3-compatible storage.</p>\n</li>\n<li>\n<p>You have a created a pipeline in JupyterLab.</p>\n</li>\n<li>\n<p>You have opened your pipeline in the Pipeline Editor in JupyterLab.</p>\n</li>\n<li>\n<p>Your pipeline instance contains a runtime configuration.</p>\n</li>\n<li>\n<p>You have created and launched a Jupyter server from a notebook image that contains the Elyra extension (Standard data science, TensorFlow, TrustyAI, or  PyTorch).</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>In the Pipeline Editor user interface, click <strong>Export Pipeline</strong> (<span class=\"image\"><img src=\"/static/docs/images/jupyterlab-export-pipeline-button.png\" alt=\"Export pipeline\"></span>).</p>\n<div class=\"paragraph\">\n<p>The <strong>Export Pipeline</strong> dialog appears. The <strong>Pipeline Name</strong> field is automatically populated with the pipeline file name.</p>\n</div>\n</li>\n<li>\n<p>Define the settings to export your pipeline.</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>From the <strong>Runtime Configuration</strong> list, select the relevant runtime configuration to export your pipeline.</p>\n</li>\n<li>\n<p>From the <strong>Export Pipeline as</strong> select an appropriate file format</p>\n</li>\n<li>\n<p>In the <strong>Export Filename</strong> field, enter a file name for the exported pipeline.</p>\n</li>\n<li>\n<p>Select the <strong>Replace if file already exists</strong> check box to replace an existing file of the same name as the pipeline you are exporting.</p>\n</li>\n<li>\n<p>Optional: Configure your pipeline parameters, if applicable. If your pipeline contains nodes that reference pipeline parameters, you can change the default parameter values. If a parameter is required and has no default value, you must enter a value.</p>\n</li>\n</ol>\n</div>\n</li>\n<li>\n<p>Click <strong>OK</strong>.</p>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>You can view the file containing the pipeline that you exported in your designated object storage bucket.</p>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>\n<div class=\"sect1 _additional-resources\">\n<h2 id=\"_additional_resources\">Additional resources</h2>\n<div class=\"sectionbody\">\n<div class=\"ulist\">\n<ul>\n<li>\n<p><a href=\"https://www.kubeflow.org/docs/components/pipelines/v1/\">Kubeflow Pipelines v1 Documentation</a>\n<a href=\"{odhdocshome}{default-format-url}/working_on_data_science_projects/working-with-data-science-pipelines_ds-pipelines#working-with-pipelines-in-jupyterlab\">Working with pipelines in JupyterLab</a>.</p>\n</li>\n</ul>\n</div>\n</div>\n</div>","id":"650137f6-3c6a-5d64-821b-48d3f5246053","document":{"title":"Working with data science pipelines"}},"markdownRemark":null},"pageContext":{"id":"650137f6-3c6a-5d64-821b-48d3f5246053"}},"staticQueryHashes":["2604506565"],"slicesMap":{}}